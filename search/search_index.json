{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Introduction Methane monitoring has reached an inflection point. Multispectral and hyperspectral satellites now revisit every major production basin multiple times per week, delivering raw data with the spatial and spectral resolution to resolve individual methane emission plumes. Translating that stream of pixels into quantified emission detections, still demands a complex tool\u2011chain and deep domain knowledge across satellite data process ing, advanced machine learning, radiative transfer physics, synthetic\u2011data generation and many more. The industry has got to a point where different providers of data will deliver different outputs (detections, quantifications, etc) when processing the exact same raw satellite imagery. The siloed nature of development in the industry is creating confusion and in some cases distrust amongst those who would benefit most from high quality emissions data. Project\u202fEucalyptus is an attempt to close that gap. We are publishing a comprehensive tutorial to how Orbio has built its end-to-end methane pipelines over the last couple of years. Next to an extensive Q&A-like documentation, we are sharing large amounts of Orbio\u202fEarth\u2019s production\u2011grade methane\u2011analytics pipeline\u2014models, tooling, validation notebooks and reference datasets\u2014released as open source under a non\u2011commercial licence. We are placing on the public square the same tools we use every day to alert producers when a thief hatch is left open or a flare goes out. No screenshots, no \u201crequest a demo\u201d\u2014actual weight files, notebooks and reproducible experiments. What is this doc? A Q&A-like tutorial covering a wide range of lessons learned across training data, model training, tiling grids, cloud masks, validation of models, plume masking and quantification. In the wider repo , and code dump , you will find: Trained models for Sentinel\u20112, Landsat\u202f8/9 and EMIT, exportable to any modern framework. End\u2011to\u2011end notebooks & scripts covering inference, post\u2011processing, and objective detection\u2011threshold evaluation. Synthetic\u2011plume toolkit that injects physically realistic methane signatures into clean scenes for training and stress\u2011testing. All components are released under a non\u2011commercial licence, see here . Commercial users: talk to us\u2014there\u2019s plenty we can build together. Contact: info@orbio.earth Why are we publishing this? Opening the Black Box. Operators, investors and regulators increasingly rely on satellite methane estimates to steer day-to-day decisions. But as methane is a really hard problem and current best practice models are still known to cause issues (e.g. false positives or high error bars), users deserve to inspect the math behind the numbers. Methodological convergence. Today, every team reinvents radiative transfer tricks, cloud masks and performance evaluation protocols. By publishing our approach to many of the complex problems in methane and satellites, we want to support a dialog towards methodological convergence and help bring everybody closer to \u201cthe truth\u201d. Accelerate innovation. The synthetic\u2011plume engine and ready\u2011trained networks let newcomers skip months to years of plumbing and jump straight to new ideas\u2014new models, better ways of benchmarking quantification accuracy, and casual discoveries. Progress compounds when the baseline is shared. Planet\u2011scale impact. The climate clock is loud. Opening our stack is the fastest way to build trust, accelerate improvements to models and multiply our impact. Who is Orbio? Orbio Earth is a climate-tech company founded in 2021 to turn open satellite data into actionable methane intelligence. Project Eucalyptus is authored by the Orbio Earth team\u2014Maxime Rischard, Timothy Davis, Zani Fooy, Mahrukh Niazi, Philip Popien, Robert Edwards, Vikram Singh, Jack Angela, Thomas Edwards, Maria Navarette, Diehl Sillers, Wojciech Adamczyk and Robert Huppertz\u2014 with support from external collaborators.","title":"Introduction"},{"location":"index.html#introduction","text":"Methane monitoring has reached an inflection point. Multispectral and hyperspectral satellites now revisit every major production basin multiple times per week, delivering raw data with the spatial and spectral resolution to resolve individual methane emission plumes. Translating that stream of pixels into quantified emission detections, still demands a complex tool\u2011chain and deep domain knowledge across satellite data process ing, advanced machine learning, radiative transfer physics, synthetic\u2011data generation and many more. The industry has got to a point where different providers of data will deliver different outputs (detections, quantifications, etc) when processing the exact same raw satellite imagery. The siloed nature of development in the industry is creating confusion and in some cases distrust amongst those who would benefit most from high quality emissions data. Project\u202fEucalyptus is an attempt to close that gap. We are publishing a comprehensive tutorial to how Orbio has built its end-to-end methane pipelines over the last couple of years. Next to an extensive Q&A-like documentation, we are sharing large amounts of Orbio\u202fEarth\u2019s production\u2011grade methane\u2011analytics pipeline\u2014models, tooling, validation notebooks and reference datasets\u2014released as open source under a non\u2011commercial licence. We are placing on the public square the same tools we use every day to alert producers when a thief hatch is left open or a flare goes out. No screenshots, no \u201crequest a demo\u201d\u2014actual weight files, notebooks and reproducible experiments.","title":"Introduction"},{"location":"index.html#what-is-this-doc","text":"A Q&A-like tutorial covering a wide range of lessons learned across training data, model training, tiling grids, cloud masks, validation of models, plume masking and quantification. In the wider repo , and code dump , you will find: Trained models for Sentinel\u20112, Landsat\u202f8/9 and EMIT, exportable to any modern framework. End\u2011to\u2011end notebooks & scripts covering inference, post\u2011processing, and objective detection\u2011threshold evaluation. Synthetic\u2011plume toolkit that injects physically realistic methane signatures into clean scenes for training and stress\u2011testing. All components are released under a non\u2011commercial licence, see here . Commercial users: talk to us\u2014there\u2019s plenty we can build together. Contact: info@orbio.earth","title":"What is this doc?"},{"location":"index.html#why-are-we-publishing-this","text":"Opening the Black Box. Operators, investors and regulators increasingly rely on satellite methane estimates to steer day-to-day decisions. But as methane is a really hard problem and current best practice models are still known to cause issues (e.g. false positives or high error bars), users deserve to inspect the math behind the numbers. Methodological convergence. Today, every team reinvents radiative transfer tricks, cloud masks and performance evaluation protocols. By publishing our approach to many of the complex problems in methane and satellites, we want to support a dialog towards methodological convergence and help bring everybody closer to \u201cthe truth\u201d. Accelerate innovation. The synthetic\u2011plume engine and ready\u2011trained networks let newcomers skip months to years of plumbing and jump straight to new ideas\u2014new models, better ways of benchmarking quantification accuracy, and casual discoveries. Progress compounds when the baseline is shared. Planet\u2011scale impact. The climate clock is loud. Opening our stack is the fastest way to build trust, accelerate improvements to models and multiply our impact.","title":"Why are we publishing this?"},{"location":"index.html#who-is-orbio","text":"Orbio Earth is a climate-tech company founded in 2021 to turn open satellite data into actionable methane intelligence. Project Eucalyptus is authored by the Orbio Earth team\u2014Maxime Rischard, Timothy Davis, Zani Fooy, Mahrukh Niazi, Philip Popien, Robert Edwards, Vikram Singh, Jack Angela, Thomas Edwards, Maria Navarette, Diehl Sillers, Wojciech Adamczyk and Robert Huppertz\u2014 with support from external collaborators.","title":"Who is Orbio?"},{"location":"Data.html","text":"Data What is the ground truth data used to train a neural network to detect methane in Sentinel 2 satellite images? The main obstacle to using computer vision for methane detection in satellite imagery is the lack of ground truth data. The only reliable sources would be airborne surveys which use hyperspectral instruments to map methane concentrations (such as AVIRIS). However, there are three obstacles to using this data: it is still a small dataset, on the order of a few thousand observations some of the plumes are much weaker than what can be detected from a satellite the odds are very low that a satellite overpass will have occurred at the exact moment the airborne observation was made Unlike other common computer vision tasks, human labelling of data is also impossible. This is not an easy task for the human eye and brain, unlike recognising dogs and cats or semantic segmentation of objects in photographs. Methane manifests in these images as a very weak and diffuse absorbance in two of the Sentinel 2 bands. At best, human labelling could be used for segmentation or classification (methane vs. false positive) of plumes found in the output of the classical algorithms. But in our day to day work of looking at this output, we find that many candidate plumes are ambiguous. We typically use a few indicators of whether a detection is likely to be real: proximity to oil and gas infrastructure, the morphology of the plume, and alignment with wind direction. But these assessments are subjective and unreliable. But all is not lost, because the physics of light absorption by methane is very well understood, and easy to emulate. This makes creating synthetic data feasible. The idea is to (1) start with a methane-free Sentinel 2 scene, (2) pick a source pixel and methane emission rate, (3) simulate the time-evolution of the resulting plume over a number of hours with some input wind field (4) modify the methane-free Sentinel 2 bands using the radiative transfer algorithm. Following these four steps a million times in a million different places would in theory give us the perfect training dataset. Each of these steps is possible, and have been done before, but step (3) is computationally extremely expensive. This is the domain of \"large eddy simulations\" (LES), numerical atmospheric models that approximate atmospheric physics step-by-step on a four-dimensional grid (time, altitude, x and y coordinates). Javier Gorro\u00f1o in Daniel Varon's group at Harvard have run a handful of such simulations on university super-computers, and these are available (we use them as part of our test suite), see Gorro\u00f1o et al. 2023. Some groups such as Radman et al. (2023) use a small number of LES simulations (10 in this case) sampled repeatedly in time to form a corpus of training plumes. These plumes are used repeatedly over a variety of satellite scenes to train neural networks. Even this limited number of simulations is complex and expensive, but the generalizability of this approach is risky, as the neural network could learn to recognise certain plume shapes in different contexts, but fail to detect methane in real life. To get away from this high computational cost, one possibility is to use much coarser and approximate simulations. For example, one could use a Gaussian air pollutant dispersion model to emulate methane in the atmosphere from a point source, as was demonstrated in Rouet-Leduc and Hulbert (2024). Alexandre et al. 2025 also use a similar strategy for the related problem of detecting black carbon in flares. There is a risk with this approach, which is that the neural network could learn to recognise particular morphological features of that approximation that generalise poorly to the real world. A certain level of realism therefore needs to be achieved in order to detect real methane plumes. We describe one approach in more details in How do you simulate methane plumes? How do you simulate methane plumes? We simulate methane plumes using a custom 2D Gaussian puff model, a standard approach for modeling how pollutants disperse in the atmosphere. Dean (2015) gives a bit of background on Gaussian plume and Gaussian puff models, and provides further references. The model treats a continuous methane release as a series of individual \"puffs\" of gas. Each puff is carried by the wind field and diffuses/spreads out over time, forming a Gaussian concentration profile. To create realistic simulations, our model incorporates several key features: Time-varying Wind: instead of assuming a constant wind speed and direction, we use real sub-hourly wind data, though it could also be simulated. This allows the plume to meander and change direction, mimicking real-world conditions. Turbulence: to simulate the small-scale, chaotic motions of the atmosphere, we add a random \"wobble\" to the position of each puff using a Ornstein\u2013Uhlenbeck random walk. This creates a more complex and realistic plume structure. All plumes were simulated with an emission rate of 1000 kg/hr, but can be rescaled by an arbitrary multiplicative factor before inserting them into a satellite scene. We have experimented with Pareto distributions, to mimic the power law of plume sizes that have been observed in multiple sources such as Ehret et al. 2022, but found that the very high skew of that distribution gave the model insufficient training examples in the 500-2,000 kg/hr range, which caused the model performance to deteriorate. We also experimented with log-normal distributions with a scale parameter of 1,000 kg/hr. Currently, we do not have a strong recommendation for the best distribution to use for model training. We calibrated the parameters of the Gaussian puff model using the five large eddy simulation outputs of methane plumes provided by Gorro\u00f1o et al. (2023). Have you tried other simulation techniques for methane plumes? Orbio's early computer vision prototypes were trained using what we called \"recycled plumes.\" These were plumes retrieved and masked out of Sentinel 2 imagery using classical methods (derived from Varon et al. (2021)), and without any of our filters for false positives. The vast majority of these are therefore false positives, triggered by biophysical processes that affect the B12 and B11 reflectances. Using false positives as \"ground truth\" was a deliberate strategy to eliminate morphological information that we do not want the neural network to use. Since the simulated ground truth mimics false positives, the neural network is forced to use alternative information from the Sentinel 2 bands, and is prevented from using \"morphology shortcuts\" that could fail to generalise. For example, the neural network can learn the fact that a methane plume can never have a corresponding shape that appears in the other bands, as methane is transparent in all other bands. From the radiative transfer equations, it can learn the distinct spectral signature of methane, and it can learn that methane can only absorb light, and cannot make a pixel brighter. It can learn to recognise and filter out common objects and phenomena that often cause false positives with the classical algorithm: water bodies, clouds, cloud shadows, etc. Figure: detection of a plume at Hassi Messaoud with a computer vision model trained using \"recycled\" plumes only. This strategy was quite successful, and has the distinctive advantage that morphology can still be used for quality assurance purposes, whereas models trained on realistic plumes could potentially \"hallucinate\" plumes that look very realistic. We show an example below from a model trained with Gaussian plumes. Figure: a very realistic-looking false positive from our validation set observed in an intermediary epoch of one of our models trained with Gaussian plumes. The major downside of using \"recycled\" plumes is that morphological information is left unused, information that is very helpful for the neural network to distinguish between real methane and false positives. Lowering the detection threshold therefore required us to insert realistic plumes in the synthetic data. We found that inserting rescaled plumes detected by AVIRIS yielded some of our best results to date, and is therefore what was used to train the model supplied within this repository. There are a few caveats to keep in mind. First, the retrieval and masking of these plumes is the product of another algorithm (generally a matched filter) and reflects choices made to filter plumes from false positives. This could introduce biases in the training data, for example towards larger plumes, or poor masking in certain cases distorting the morphological information. There is also a relatively limited number (a few thousand) of such plumes available for training, which creates a risk of overfitting. For these reasons, we believe there may still be benefits from using simulated plumes, but our initial experiments with Gaussian plumes have not yet yielded clear improvements. What is the input to the neural network? The input (X) to the neural network is a selection of Sentinel 2 (L1C) reflectance bands from the target scene and two prior reference scenes of the same location, concatenated into a single tensor. In our latest models, we use bands B11, B12, B8A, B07, B05, B04, B03, and B02, and crop 128x128 pixels for each chip, so the input tensor has shape 24x128x128. We use the top of the atmosphere reflectances directly from the L1C product supplied by Copernicus. The raw numbers from the L1C granules are rescaled so the reflectance scale is in the 0-1 range. We discuss the alternative use for L2A granules in Have you tried using your model on Level 2A Sentinel 2 data? . For each chip, the labels (y) are the fractional change in the B12/B11 band ratio (\"frac\") from the methane-free scene (denoted by a subscript o) to the observed reflectances. \\[ y = \\frac{B12 / B11}{B12_o / B11_o} - 1 \\] Frac is zero for pixels without methane, and negative when methane is present. It is known for scenes with synthetically inserted methane, but otherwise must be estimated. Why do you use reference scenes? We include reference scenes to give the model temporal context\u2014 a view of what the area typically looks like without methane. This helps the model focus on changes that could indicate methane, rather than focus on terrain/other background features. The idea is conceptually similar to the Multi-Band Multi-Pass (MBMP) approach (Varon et al. 2021) used in the classical physics-based methane detection, where multiple past observations are compared to a target scene (with methane). Like MBMP, our method uses temporal context to enhance the methane signal and reduce false positives, but instead of just computing the differences between the spectral bands, we learn these changes using supervised learning. Have you tried using your model on Level 2A Sentinel 2 data? The vast majority of the literature on methane detection in multispectral data uses L1C (top of the atmosphere reflectance) data, which is conceptually natural as the absorption of light due to methane happens in the atmosphere. The post-processing of the L1C to L2A aims to invert atmospheric effects to obtain the surface reflectance. One may assume that this should therefore remove the effect of methane plumes in the atmosphere, but of course the presence of a methane plume is unknown to the processing software, so it turns out L2A data can also be used. Our experiments training and validating computer vision algorithms on L2A data showed no significant difference in performance. In the end, we opted for L1C due to the earlier availability of the data for real-time use cases, and to mitigate any remaining doubts that the L2A processing could distort the methane signal. But for other use cases, using L2A data could have advantages. In particular, the wide availability of L2A data as cloud-optimised geotiffs could be computationally advantageous for targeted inference on small areas. How do you select reference scenes? For each target scene, our model uses the two last cloud/cloud shadow free images closest in time as additional reference scenes to make detecting methane easier. Because having a nearly cloud free reference scene within the last week of the target scene is much better than having a completely cloud free reference scene one month in the past, we allow for up to 5% of \u201cbad pixels\u201d, meaning clouds, cloud shadows or no data. The larger the crop of the whole satellite image we use as a target scene, the harder it is to find perfect, low cloud reference scenes as parts of the image may be cloudy and other parts are clear. Hence, choosing crop sizes not larger than e.g. 500x500 px is advised to be able to find clear view reference scenes close in time. We use 128x128 px crops during training and 500x500 px crops in production. The whole synthetic data generation pipeline can be inspected here . Why are the training labels not just the methane concentration? The choice of metric for the \"ground truth\" labels is a fairly arbitrary choice. For example, the fractional absorption in B12 would also be suitable. However, we do not predict the methane enhancement (in mol/m\u00b2, say) directly, because obtaining this requires additional information about the angle of the sun and of the instrument and assumptions about the atmospheric composition that are not made available to the neural network. Translating the frac into methane concentration is a simple postprocessing step that we describe in more detail in Postprocessing . We made a different choice for hyperspectral data, see Hyperspectral . How large are your training and validation datasets? We used a training dataset of around 1.5 million 128x128 chips coming from 3413 Sentinel 2 IDs and 1407 different MGRS tiles overlapping global oil and gas producing areas. The validation dataset had around 225,000 128x128 chips coming from 248 Sentinel 2 IDs and 33 different MGRS tiles. The validation data was selected to come from three important and very different regions in terms of vegetation and biome classification: Hassi (Algeria, desert), Permian (Texas, New Mexico, arid to semi-arid) and Marcellus (North East of the USA, temperate forests). Blue: MGRS tiles used for training data, Red: MGRS tiles used for validation data How was the training dataset created? We started by filtering all global MGRS Sentinel 2 tiles down to those intersecting oil and gas producing areas using an internal dataset. Then, given a number of total IDs we want to sample and a distribution over world region, we queried the Sentinel 2 Planetary Computer STAC catalogue for random IDs in the selected MGRS tiles over the 2016-2024 history. The distribution over world regions should be selected according to where the models are applied in production and where how much oil and gas is produced. Ours was: 34% US, 24% Middle East, 12% Africa, 10% Commonwealth of Independent States (CIS), 5% NA without USA, 5% South and Middle America, 5% Asia, 2.5% Australia, 2.5% EU As we want the model to be able to deal with all kinds of conditions, clouds and no data, we did not restrict the initial querying for IDs in any significant way. Only after an ID is sampled with X% obscured pixels (e.g. 80% where 50% are no data and another 30% clouds), we sample another ID with probability X% with a maximum of 40% no data and 40% cloud cover (in the valid pixels). This is done to ensure that we have valid, non-obscured pixels from all sampled areas. This whole process can be inspected within this notebook . How was the validation dataset created? The validation data was selected to come from three important and very different regions in terms of vegetation and biome classification: Hassi (Algeria, desert), Permian (Texas, New Mexico, arid to semi-arid) and Marcellus (North East of the USA, temperate forests). See here for more details on the choice of these regions. We sampled much more (around 8) Sentinel 2 IDs for each of the 66 MGRS tiles covering these three regions from 2016-2024 and then split off 50% of the MGRS tiles per region and their IDs into the training set and the other 50% into the validation set. Splitting on MGRS level made sure that the spatial overlap is minimal which makes the validation results robust against spatial overfitting. The resulting validation set has 33 MGRS tiles (9 from the region around Hassi, 13 from Marcellus and 11 from Permian) and 248 Sentinel 2 IDs from 2016-2022. This whole process can be inspected within this notebook . Have you tried using a larger training dataset? We gradually increased the size of the training dataset in early experiments, but found that the benefits of doing so tapered off at the current size. We find that halving or doubling the dataset size does not have a measurable effect on model performance. This is unusual in deep learning applications, but could be explained by our already large dataset (around 1.5million chips), we have seen significant improvements going from 0.1million to 0.75million chips. What cloud mask do you use? We recently switched from using the Scene Classification Layer of Sentinel-2 L2A to using OmniCloudMask which is slower (as it\u2019s using Deep Learning models itself), but much more accurate in cloud and cloud shadow segmentation, see the Figure below. Left: OmniCloud clouds and cloud shadows, Right: SCL clouds and cloud shadows for a full Sentinel-2 MGRS scene How is the data saved and loaded back in for training? Our experience with geospatial data is that common formats like GeoTIFFs, NetCDF or JPEG2000 (the native format for the Sentinel 2 L1C data) incur a high overhead when loading the data into memory. Benchmarking the time taken to load a certain file into memory using common tools like GDAL and rasterio, and comparing them to the read speeds of the underlying disk or file system, usually shows at least an order of magnitude discrepancy. This is especially true for small files, like our chips for training neural networks. We performed some benchmarks reading and writing to disk a random 64 bit floats in an array of size 8x256x256. The `numpy.tobytes` and `numpy.frombuffer` methods, which simply dump the memory representations of the arrays onto disk, took 82 \u00b5s and 1.1 \u00b5s. Meanwhile, writing to a netcdf3 file with xarray took 3.3\u00d710\u00b3 \u00b5s and reading took 9.5\u00d710\u00b2 \u00b5s. Using the zarr file format or h5netcdf were even slower. For this reason, we decided to use `numpy.tobytes` directly for saving training chips. We also decided to store the training chips in large parquet files, each containing the chips for a single Sentinel 2 L1C granule. The idea was to avoid having millions of tiny files, and to be able to store metadata (like the number of plumes inserted) alongside the features and labels, and use a cloud-native format to be able to randomly access rows, so we could easily shuffle the training data between epochs. The parquet file format turned out to be inappropriate for this case, and we would not recommend replicating this pattern. Setting the row group small enough to enable efficient random access (we set the row group size to be just one row) made the size of the parquet metadata balloon, so just opening the parquet files without reading in any data resulted in out-of-memory errors crashing the training process. For loading the data into pytorch, our attempts to leverage `dask` proved fruitless, and we wrote custom Dataset classes wrapping `pyarrow` methods directly to enable efficient random access over multiple files. We also found that we needed to use pytorch's DistributedDataParallel to fully utilize the multiple GPUs on our training instances. Are any data augmentations used during training? Unlike most computer vision applications, because we can generate arbitrarily large synthetic datasets, using augmentations to juice as much information as possible out of every sample is not essential. So we only use simple non-destructive augmentations: random rotations by multiples of 90\u00b0, and random flips. In earlier model training, we also implemented and used a signal modulation transformation that weakens the methane concentration by a random constant factor in each chip. We found that presenting the model with stronger plumes in early epochs, and gradually reducing them in later epochs, helped guide the optimizer. This can be understood as a form of curriculum learning (Bengio 2009). The randomness was important so the model would still occasionally see strong examples, as otherwise the model would forget how to detect strong plumes in later epochs. In more recent training runs, with improvements to the optimizer settings and model architecture, we noticed that the modulation was no longer needed to get good results, and the modulation settings had no significant effect on model performance, so we are no longer using this transformation. References Alexandre, Tuel, Kerdreux Thomas, and Thiry Louis. \"Black carbon plumes from gas flaring in North Africa identified from multi-spectral imagery with deep learning.\" arXiv preprint arXiv:2406.06183 (2024). Bengio, Yoshua, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. \"Curriculum learning.\" In Proceedings of the 26th annual international conference on machine learning , pp. 41-48. 2009. Dean, Christopher Lee. \"Efficient MCMC inference for remote sensing of emission sources.\" PhD diss., Massachusetts Institute of Technology, 2015. Ehret, Thibaud, Aur\u00e9lien De Truchis, Matthieu Mazzolini, Jean-Michel Morel, Alexandre D\u2019aspremont, Thomas Lauvaux, Riley Duren, Daniel Cusworth, and Gabriele Facciolo. \"Global tracking and quantification of oil and gas methane emissions from recurrent sentinel-2 imagery.\" Environmental science & technology 56, no. 14 (2022): 10517-10529. Gorro\u00f1o, Javier, Daniel J. Varon, Itziar Irakulis-Loitxate, and Luis Guanter. \"Understanding the potential of Sentinel-2 for monitoring methane point emissions.\" Atmospheric Measurement Techniques 16, no. 1 (2023): 89-107. Radman, Ali, Masoud Mahdianpari, Daniel J. Varon, and Fariba Mohammadimanesh. \"S2MetNet: A novel dataset and deep learning benchmark for methane point source quantification using Sentinel-2 satellite imagery.\" Remote Sensing of Environment 295 (2023): 113708. Varon, Daniel J., Dylan Jervis, Jason McKeever, Ian Spence, David Gains, and Daniel J. Jacob. \"High-frequency monitoring of anomalous methane point sources with multispectral Sentinel-2 satellite observations.\" Atmospheric Measurement Techniques 14, no. 4 (2021).","title":"Data"},{"location":"Data.html#data","text":"","title":"Data"},{"location":"Data.html#what-is-the-ground-truth-data-used-to-train-a-neural-network-to-detect-methane-in-sentinel-2-satellite-images","text":"The main obstacle to using computer vision for methane detection in satellite imagery is the lack of ground truth data. The only reliable sources would be airborne surveys which use hyperspectral instruments to map methane concentrations (such as AVIRIS). However, there are three obstacles to using this data: it is still a small dataset, on the order of a few thousand observations some of the plumes are much weaker than what can be detected from a satellite the odds are very low that a satellite overpass will have occurred at the exact moment the airborne observation was made Unlike other common computer vision tasks, human labelling of data is also impossible. This is not an easy task for the human eye and brain, unlike recognising dogs and cats or semantic segmentation of objects in photographs. Methane manifests in these images as a very weak and diffuse absorbance in two of the Sentinel 2 bands. At best, human labelling could be used for segmentation or classification (methane vs. false positive) of plumes found in the output of the classical algorithms. But in our day to day work of looking at this output, we find that many candidate plumes are ambiguous. We typically use a few indicators of whether a detection is likely to be real: proximity to oil and gas infrastructure, the morphology of the plume, and alignment with wind direction. But these assessments are subjective and unreliable. But all is not lost, because the physics of light absorption by methane is very well understood, and easy to emulate. This makes creating synthetic data feasible. The idea is to (1) start with a methane-free Sentinel 2 scene, (2) pick a source pixel and methane emission rate, (3) simulate the time-evolution of the resulting plume over a number of hours with some input wind field (4) modify the methane-free Sentinel 2 bands using the radiative transfer algorithm. Following these four steps a million times in a million different places would in theory give us the perfect training dataset. Each of these steps is possible, and have been done before, but step (3) is computationally extremely expensive. This is the domain of \"large eddy simulations\" (LES), numerical atmospheric models that approximate atmospheric physics step-by-step on a four-dimensional grid (time, altitude, x and y coordinates). Javier Gorro\u00f1o in Daniel Varon's group at Harvard have run a handful of such simulations on university super-computers, and these are available (we use them as part of our test suite), see Gorro\u00f1o et al. 2023. Some groups such as Radman et al. (2023) use a small number of LES simulations (10 in this case) sampled repeatedly in time to form a corpus of training plumes. These plumes are used repeatedly over a variety of satellite scenes to train neural networks. Even this limited number of simulations is complex and expensive, but the generalizability of this approach is risky, as the neural network could learn to recognise certain plume shapes in different contexts, but fail to detect methane in real life. To get away from this high computational cost, one possibility is to use much coarser and approximate simulations. For example, one could use a Gaussian air pollutant dispersion model to emulate methane in the atmosphere from a point source, as was demonstrated in Rouet-Leduc and Hulbert (2024). Alexandre et al. 2025 also use a similar strategy for the related problem of detecting black carbon in flares. There is a risk with this approach, which is that the neural network could learn to recognise particular morphological features of that approximation that generalise poorly to the real world. A certain level of realism therefore needs to be achieved in order to detect real methane plumes. We describe one approach in more details in How do you simulate methane plumes?","title":"What is the ground truth data used to train a neural network to detect methane in Sentinel 2 satellite images?"},{"location":"Data.html#how-do-you-simulate-methane-plumes","text":"We simulate methane plumes using a custom 2D Gaussian puff model, a standard approach for modeling how pollutants disperse in the atmosphere. Dean (2015) gives a bit of background on Gaussian plume and Gaussian puff models, and provides further references. The model treats a continuous methane release as a series of individual \"puffs\" of gas. Each puff is carried by the wind field and diffuses/spreads out over time, forming a Gaussian concentration profile. To create realistic simulations, our model incorporates several key features: Time-varying Wind: instead of assuming a constant wind speed and direction, we use real sub-hourly wind data, though it could also be simulated. This allows the plume to meander and change direction, mimicking real-world conditions. Turbulence: to simulate the small-scale, chaotic motions of the atmosphere, we add a random \"wobble\" to the position of each puff using a Ornstein\u2013Uhlenbeck random walk. This creates a more complex and realistic plume structure. All plumes were simulated with an emission rate of 1000 kg/hr, but can be rescaled by an arbitrary multiplicative factor before inserting them into a satellite scene. We have experimented with Pareto distributions, to mimic the power law of plume sizes that have been observed in multiple sources such as Ehret et al. 2022, but found that the very high skew of that distribution gave the model insufficient training examples in the 500-2,000 kg/hr range, which caused the model performance to deteriorate. We also experimented with log-normal distributions with a scale parameter of 1,000 kg/hr. Currently, we do not have a strong recommendation for the best distribution to use for model training. We calibrated the parameters of the Gaussian puff model using the five large eddy simulation outputs of methane plumes provided by Gorro\u00f1o et al. (2023).","title":"How do you simulate methane plumes?"},{"location":"Data.html#have-you-tried-other-simulation-techniques-for-methane-plumes","text":"Orbio's early computer vision prototypes were trained using what we called \"recycled plumes.\" These were plumes retrieved and masked out of Sentinel 2 imagery using classical methods (derived from Varon et al. (2021)), and without any of our filters for false positives. The vast majority of these are therefore false positives, triggered by biophysical processes that affect the B12 and B11 reflectances. Using false positives as \"ground truth\" was a deliberate strategy to eliminate morphological information that we do not want the neural network to use. Since the simulated ground truth mimics false positives, the neural network is forced to use alternative information from the Sentinel 2 bands, and is prevented from using \"morphology shortcuts\" that could fail to generalise. For example, the neural network can learn the fact that a methane plume can never have a corresponding shape that appears in the other bands, as methane is transparent in all other bands. From the radiative transfer equations, it can learn the distinct spectral signature of methane, and it can learn that methane can only absorb light, and cannot make a pixel brighter. It can learn to recognise and filter out common objects and phenomena that often cause false positives with the classical algorithm: water bodies, clouds, cloud shadows, etc. Figure: detection of a plume at Hassi Messaoud with a computer vision model trained using \"recycled\" plumes only. This strategy was quite successful, and has the distinctive advantage that morphology can still be used for quality assurance purposes, whereas models trained on realistic plumes could potentially \"hallucinate\" plumes that look very realistic. We show an example below from a model trained with Gaussian plumes. Figure: a very realistic-looking false positive from our validation set observed in an intermediary epoch of one of our models trained with Gaussian plumes. The major downside of using \"recycled\" plumes is that morphological information is left unused, information that is very helpful for the neural network to distinguish between real methane and false positives. Lowering the detection threshold therefore required us to insert realistic plumes in the synthetic data. We found that inserting rescaled plumes detected by AVIRIS yielded some of our best results to date, and is therefore what was used to train the model supplied within this repository. There are a few caveats to keep in mind. First, the retrieval and masking of these plumes is the product of another algorithm (generally a matched filter) and reflects choices made to filter plumes from false positives. This could introduce biases in the training data, for example towards larger plumes, or poor masking in certain cases distorting the morphological information. There is also a relatively limited number (a few thousand) of such plumes available for training, which creates a risk of overfitting. For these reasons, we believe there may still be benefits from using simulated plumes, but our initial experiments with Gaussian plumes have not yet yielded clear improvements.","title":"Have you tried other simulation techniques for methane plumes?"},{"location":"Data.html#what-is-the-input-to-the-neural-network","text":"The input (X) to the neural network is a selection of Sentinel 2 (L1C) reflectance bands from the target scene and two prior reference scenes of the same location, concatenated into a single tensor. In our latest models, we use bands B11, B12, B8A, B07, B05, B04, B03, and B02, and crop 128x128 pixels for each chip, so the input tensor has shape 24x128x128. We use the top of the atmosphere reflectances directly from the L1C product supplied by Copernicus. The raw numbers from the L1C granules are rescaled so the reflectance scale is in the 0-1 range. We discuss the alternative use for L2A granules in Have you tried using your model on Level 2A Sentinel 2 data? . For each chip, the labels (y) are the fractional change in the B12/B11 band ratio (\"frac\") from the methane-free scene (denoted by a subscript o) to the observed reflectances. \\[ y = \\frac{B12 / B11}{B12_o / B11_o} - 1 \\] Frac is zero for pixels without methane, and negative when methane is present. It is known for scenes with synthetically inserted methane, but otherwise must be estimated.","title":"What is the input to the neural network?"},{"location":"Data.html#why-do-you-use-reference-scenes","text":"We include reference scenes to give the model temporal context\u2014 a view of what the area typically looks like without methane. This helps the model focus on changes that could indicate methane, rather than focus on terrain/other background features. The idea is conceptually similar to the Multi-Band Multi-Pass (MBMP) approach (Varon et al. 2021) used in the classical physics-based methane detection, where multiple past observations are compared to a target scene (with methane). Like MBMP, our method uses temporal context to enhance the methane signal and reduce false positives, but instead of just computing the differences between the spectral bands, we learn these changes using supervised learning.","title":"Why do you use reference scenes?"},{"location":"Data.html#have-you-tried-using-your-model-on-level-2a-sentinel-2-data","text":"The vast majority of the literature on methane detection in multispectral data uses L1C (top of the atmosphere reflectance) data, which is conceptually natural as the absorption of light due to methane happens in the atmosphere. The post-processing of the L1C to L2A aims to invert atmospheric effects to obtain the surface reflectance. One may assume that this should therefore remove the effect of methane plumes in the atmosphere, but of course the presence of a methane plume is unknown to the processing software, so it turns out L2A data can also be used. Our experiments training and validating computer vision algorithms on L2A data showed no significant difference in performance. In the end, we opted for L1C due to the earlier availability of the data for real-time use cases, and to mitigate any remaining doubts that the L2A processing could distort the methane signal. But for other use cases, using L2A data could have advantages. In particular, the wide availability of L2A data as cloud-optimised geotiffs could be computationally advantageous for targeted inference on small areas.","title":"Have you tried using your model on Level 2A Sentinel 2 data?"},{"location":"Data.html#how-do-you-select-reference-scenes","text":"For each target scene, our model uses the two last cloud/cloud shadow free images closest in time as additional reference scenes to make detecting methane easier. Because having a nearly cloud free reference scene within the last week of the target scene is much better than having a completely cloud free reference scene one month in the past, we allow for up to 5% of \u201cbad pixels\u201d, meaning clouds, cloud shadows or no data. The larger the crop of the whole satellite image we use as a target scene, the harder it is to find perfect, low cloud reference scenes as parts of the image may be cloudy and other parts are clear. Hence, choosing crop sizes not larger than e.g. 500x500 px is advised to be able to find clear view reference scenes close in time. We use 128x128 px crops during training and 500x500 px crops in production. The whole synthetic data generation pipeline can be inspected here .","title":"How do you select reference scenes?"},{"location":"Data.html#why-are-the-training-labels-not-just-the-methane-concentration","text":"The choice of metric for the \"ground truth\" labels is a fairly arbitrary choice. For example, the fractional absorption in B12 would also be suitable. However, we do not predict the methane enhancement (in mol/m\u00b2, say) directly, because obtaining this requires additional information about the angle of the sun and of the instrument and assumptions about the atmospheric composition that are not made available to the neural network. Translating the frac into methane concentration is a simple postprocessing step that we describe in more detail in Postprocessing . We made a different choice for hyperspectral data, see Hyperspectral .","title":"Why are the training labels not just the methane concentration?"},{"location":"Data.html#how-large-are-your-training-and-validation-datasets","text":"We used a training dataset of around 1.5 million 128x128 chips coming from 3413 Sentinel 2 IDs and 1407 different MGRS tiles overlapping global oil and gas producing areas. The validation dataset had around 225,000 128x128 chips coming from 248 Sentinel 2 IDs and 33 different MGRS tiles. The validation data was selected to come from three important and very different regions in terms of vegetation and biome classification: Hassi (Algeria, desert), Permian (Texas, New Mexico, arid to semi-arid) and Marcellus (North East of the USA, temperate forests). Blue: MGRS tiles used for training data, Red: MGRS tiles used for validation data","title":"How large are your training and validation datasets?"},{"location":"Data.html#how-was-the-training-dataset-created","text":"We started by filtering all global MGRS Sentinel 2 tiles down to those intersecting oil and gas producing areas using an internal dataset. Then, given a number of total IDs we want to sample and a distribution over world region, we queried the Sentinel 2 Planetary Computer STAC catalogue for random IDs in the selected MGRS tiles over the 2016-2024 history. The distribution over world regions should be selected according to where the models are applied in production and where how much oil and gas is produced. Ours was: 34% US, 24% Middle East, 12% Africa, 10% Commonwealth of Independent States (CIS), 5% NA without USA, 5% South and Middle America, 5% Asia, 2.5% Australia, 2.5% EU As we want the model to be able to deal with all kinds of conditions, clouds and no data, we did not restrict the initial querying for IDs in any significant way. Only after an ID is sampled with X% obscured pixels (e.g. 80% where 50% are no data and another 30% clouds), we sample another ID with probability X% with a maximum of 40% no data and 40% cloud cover (in the valid pixels). This is done to ensure that we have valid, non-obscured pixels from all sampled areas. This whole process can be inspected within this notebook .","title":"How was the training dataset created?"},{"location":"Data.html#how-was-the-validation-dataset-created","text":"The validation data was selected to come from three important and very different regions in terms of vegetation and biome classification: Hassi (Algeria, desert), Permian (Texas, New Mexico, arid to semi-arid) and Marcellus (North East of the USA, temperate forests). See here for more details on the choice of these regions. We sampled much more (around 8) Sentinel 2 IDs for each of the 66 MGRS tiles covering these three regions from 2016-2024 and then split off 50% of the MGRS tiles per region and their IDs into the training set and the other 50% into the validation set. Splitting on MGRS level made sure that the spatial overlap is minimal which makes the validation results robust against spatial overfitting. The resulting validation set has 33 MGRS tiles (9 from the region around Hassi, 13 from Marcellus and 11 from Permian) and 248 Sentinel 2 IDs from 2016-2022. This whole process can be inspected within this notebook .","title":"How was the validation dataset created?"},{"location":"Data.html#have-you-tried-using-a-larger-training-dataset","text":"We gradually increased the size of the training dataset in early experiments, but found that the benefits of doing so tapered off at the current size. We find that halving or doubling the dataset size does not have a measurable effect on model performance. This is unusual in deep learning applications, but could be explained by our already large dataset (around 1.5million chips), we have seen significant improvements going from 0.1million to 0.75million chips.","title":"Have you tried using a larger training dataset?"},{"location":"Data.html#what-cloud-mask-do-you-use","text":"We recently switched from using the Scene Classification Layer of Sentinel-2 L2A to using OmniCloudMask which is slower (as it\u2019s using Deep Learning models itself), but much more accurate in cloud and cloud shadow segmentation, see the Figure below. Left: OmniCloud clouds and cloud shadows, Right: SCL clouds and cloud shadows for a full Sentinel-2 MGRS scene","title":"What cloud mask do you use?"},{"location":"Data.html#how-is-the-data-saved-and-loaded-back-in-for-training","text":"Our experience with geospatial data is that common formats like GeoTIFFs, NetCDF or JPEG2000 (the native format for the Sentinel 2 L1C data) incur a high overhead when loading the data into memory. Benchmarking the time taken to load a certain file into memory using common tools like GDAL and rasterio, and comparing them to the read speeds of the underlying disk or file system, usually shows at least an order of magnitude discrepancy. This is especially true for small files, like our chips for training neural networks. We performed some benchmarks reading and writing to disk a random 64 bit floats in an array of size 8x256x256. The `numpy.tobytes` and `numpy.frombuffer` methods, which simply dump the memory representations of the arrays onto disk, took 82 \u00b5s and 1.1 \u00b5s. Meanwhile, writing to a netcdf3 file with xarray took 3.3\u00d710\u00b3 \u00b5s and reading took 9.5\u00d710\u00b2 \u00b5s. Using the zarr file format or h5netcdf were even slower. For this reason, we decided to use `numpy.tobytes` directly for saving training chips. We also decided to store the training chips in large parquet files, each containing the chips for a single Sentinel 2 L1C granule. The idea was to avoid having millions of tiny files, and to be able to store metadata (like the number of plumes inserted) alongside the features and labels, and use a cloud-native format to be able to randomly access rows, so we could easily shuffle the training data between epochs. The parquet file format turned out to be inappropriate for this case, and we would not recommend replicating this pattern. Setting the row group small enough to enable efficient random access (we set the row group size to be just one row) made the size of the parquet metadata balloon, so just opening the parquet files without reading in any data resulted in out-of-memory errors crashing the training process. For loading the data into pytorch, our attempts to leverage `dask` proved fruitless, and we wrote custom Dataset classes wrapping `pyarrow` methods directly to enable efficient random access over multiple files. We also found that we needed to use pytorch's DistributedDataParallel to fully utilize the multiple GPUs on our training instances.","title":"How is the data saved and loaded back in for training?"},{"location":"Data.html#are-any-data-augmentations-used-during-training","text":"Unlike most computer vision applications, because we can generate arbitrarily large synthetic datasets, using augmentations to juice as much information as possible out of every sample is not essential. So we only use simple non-destructive augmentations: random rotations by multiples of 90\u00b0, and random flips. In earlier model training, we also implemented and used a signal modulation transformation that weakens the methane concentration by a random constant factor in each chip. We found that presenting the model with stronger plumes in early epochs, and gradually reducing them in later epochs, helped guide the optimizer. This can be understood as a form of curriculum learning (Bengio 2009). The randomness was important so the model would still occasionally see strong examples, as otherwise the model would forget how to detect strong plumes in later epochs. In more recent training runs, with improvements to the optimizer settings and model architecture, we noticed that the modulation was no longer needed to get good results, and the modulation settings had no significant effect on model performance, so we are no longer using this transformation.","title":"Are any data augmentations used during training?"},{"location":"Data.html#references","text":"Alexandre, Tuel, Kerdreux Thomas, and Thiry Louis. \"Black carbon plumes from gas flaring in North Africa identified from multi-spectral imagery with deep learning.\" arXiv preprint arXiv:2406.06183 (2024). Bengio, Yoshua, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. \"Curriculum learning.\" In Proceedings of the 26th annual international conference on machine learning , pp. 41-48. 2009. Dean, Christopher Lee. \"Efficient MCMC inference for remote sensing of emission sources.\" PhD diss., Massachusetts Institute of Technology, 2015. Ehret, Thibaud, Aur\u00e9lien De Truchis, Matthieu Mazzolini, Jean-Michel Morel, Alexandre D\u2019aspremont, Thomas Lauvaux, Riley Duren, Daniel Cusworth, and Gabriele Facciolo. \"Global tracking and quantification of oil and gas methane emissions from recurrent sentinel-2 imagery.\" Environmental science & technology 56, no. 14 (2022): 10517-10529. Gorro\u00f1o, Javier, Daniel J. Varon, Itziar Irakulis-Loitxate, and Luis Guanter. \"Understanding the potential of Sentinel-2 for monitoring methane point emissions.\" Atmospheric Measurement Techniques 16, no. 1 (2023): 89-107. Radman, Ali, Masoud Mahdianpari, Daniel J. Varon, and Fariba Mohammadimanesh. \"S2MetNet: A novel dataset and deep learning benchmark for methane point source quantification using Sentinel-2 satellite imagery.\" Remote Sensing of Environment 295 (2023): 113708. Varon, Daniel J., Dylan Jervis, Jason McKeever, Ian Spence, David Gains, and Daniel J. Jacob. \"High-frequency monitoring of anomalous methane point sources with multispectral Sentinel-2 satellite observations.\" Atmospheric Measurement Techniques 14, no. 4 (2021).","title":"References"},{"location":"Data.html#_1","text":"","title":""},{"location":"Hyperspectral.html","text":"Hyperspectral What are the key differences between the training data and model for multispectral data (Sentinel 2) and hyperspectral data (EMIT)? EMIT dataset generation generally followed the same pattern as Sentinel-2 and Landsat 8/9. EMIT granules were sampled globally for the training split, with the validation split stratified across three oil producing regions: the Permian Basin (operationally relevant), Colorado (difficult background, analogous to the Marcellus region used for the other satellites), and North Africa (easy background). More details on regional stratification are given in Validation . Some 1,120 granules were used for the training split and 199 for validation. The granules themselves were tiled into 128\u00d7128 pixel crops. Into these we synthetically inserted plumes identified by the EMIT and AVIRIS sensors. We drew from a set of 2,544 plumes for the training chips and 636 for validation, with plumes reused across their respective splits. Spatial transformations were applied to the plumes, but in contrast to the datasets of the other satellites there was no modulation of plume concentration. Another important difference in the EMIT dataset is that no temporal reference imagery is used either for training or inference. That is described in greater detail in the following section. Which of the spectral bands do you use? We have opted to use all 285 of EMIT's spectral bands. This is different from the choice made in R\u016f\u017ei\u010dka et al. (2025) for a similar model, where they only use 86 bands in the methane-absorbing part of the spectrum. This is informed by our experience with Sentinel 2, where bands outside of the methane-absorbing regions are essential to filter out false positives. Our understanding is that they act as controls \u2013 if a candidate methane plume has a corresponding morphological signature detectable outside of the methane-absorbing region, then it must be a false positive. However, for hyperspectral data, we have not experimentally validated the hypothesis that providing the additional bands results in better performance. Have you tried training a hyperspectral model with reference scenes? For a number of reasons, we have not tried training a hyperspectral model with reference scenes. EMIT has a fine spectral resolution making it well suited to the problem of methane detection, whereas Sentinel-2 and Landsat 8/9 each have two broad bands covering methane-absorbing spectra. This suggests that a machine learning approach using a single acquisition is possible for EMIT while historical reference imagery is necessary to draw out the methane signal for the other satellites. Additionally, EMIT has sparse historical coverage compared to the other satellites, meaning that high quality reference scenes may not be available for a given location or they are likely to be further removed in time. Finally, poor reference scenes can frustrate inference; the presence of unmasked clouds or an emissions event in reference imagery can cause incorrect predictions, particularly false negatives. Given this, we decided to approach the methane detection problem using a single scene (monotemporal imagery) in the case of EMIT rather than try to leverage reference imagery Results and limitations The EMIT CV model achieved the following scores on the validation set: F1 score: 0.720 Recall: 0.584 Precision: 0.938 Mean squared error: 0.099 Given that the validation set had been created by synthetically inserting plumes, we put particular weight on qualitatively evaluating an additional set of 61 known emissions identified by IMEO in their original granules, i.e. without having been synthetically inserted. As a comparison baseline we used the matched filter (MF) algorithm implementation mag1c by Foote et al . The EMIT CV model successfully detected the vast majority of these emissions events. False negatives generally correlated with at least one of the following criteria: difficult image background, low methane concentrations, or, particularly, spatially small or sparse methane-containing pixels. An example of a true positive and a false negative are shown below. Figure: Example of a true positive detection by the EMIT CV model. Figure: Example of a false negative detection by the EMIT CV model. The finding that the model misses a number of small or sparse plumes suggests that changes to the loss function could allow for an improved model. The mean squared error portion of the model is relatively insensitive to errors of a few pixels. For larger plumes this makes sense: missing only a few pixels would often be considered good enough. However, entirely missing a small plume of a few pixels is more problematic from an operational perspective given that recall is of utmost importance, especially given that at the EMIT resolution of 60 meters, small plumes will be a regular occurrence. Tweaking the loss function to account for this could be promising but has not yet been tested. Does the EMIT CV model outperform the matched filter? It would be difficult to assert that one method is outright better than the other. From the evaluation of IMEO notified plumes, the matched filter does identify methane in a number of cases where the EMIT CV model either predicts with very low confidence or misses the methane altogether. As discussed above, these are generally cases of very small or sparse plumes. The matched filter results contain considerable noise, which we have frequently found to frustrate the automatic segmentation of plumes. The EMIT CV model produces results with a much higher signal-to-noise ratio, making them easier to segment automatically. Given the advantages of each approach, the matched filter is still better suited to cases where manual review and segmentation are feasible. This will be over smaller amounts of imagery. For large-scale processing and automatic plume segmentation, the clean outputs of the EMIT CV model are preferable even if this will result in lower recall. References Foote, M. D., et al. \"Fast and Accurate Retrieval of Methane Concentration from Imaging Spectrometer Data Using Sparsity Prior\" IEEE Transactions on Geoscience and Remote Sensing. 2020. R\u016f\u017ei\u010dka, V\u00edt, and Andrew Markham. \"HyperspectralViTs: General Hyperspectral Models for On-board Remote Sensing.\" IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (2025).","title":"Hyperspectral"},{"location":"Hyperspectral.html#hyperspectral","text":"","title":"Hyperspectral"},{"location":"Hyperspectral.html#what-are-the-key-differences-between-the-training-data-and-model-for-multispectral-data-sentinel-2-and-hyperspectral-data-emit","text":"EMIT dataset generation generally followed the same pattern as Sentinel-2 and Landsat 8/9. EMIT granules were sampled globally for the training split, with the validation split stratified across three oil producing regions: the Permian Basin (operationally relevant), Colorado (difficult background, analogous to the Marcellus region used for the other satellites), and North Africa (easy background). More details on regional stratification are given in Validation . Some 1,120 granules were used for the training split and 199 for validation. The granules themselves were tiled into 128\u00d7128 pixel crops. Into these we synthetically inserted plumes identified by the EMIT and AVIRIS sensors. We drew from a set of 2,544 plumes for the training chips and 636 for validation, with plumes reused across their respective splits. Spatial transformations were applied to the plumes, but in contrast to the datasets of the other satellites there was no modulation of plume concentration. Another important difference in the EMIT dataset is that no temporal reference imagery is used either for training or inference. That is described in greater detail in the following section.","title":"What are the key differences between the training data and model for multispectral data (Sentinel 2) and hyperspectral data (EMIT)?"},{"location":"Hyperspectral.html#which-of-the-spectral-bands-do-you-use","text":"We have opted to use all 285 of EMIT's spectral bands. This is different from the choice made in R\u016f\u017ei\u010dka et al. (2025) for a similar model, where they only use 86 bands in the methane-absorbing part of the spectrum. This is informed by our experience with Sentinel 2, where bands outside of the methane-absorbing regions are essential to filter out false positives. Our understanding is that they act as controls \u2013 if a candidate methane plume has a corresponding morphological signature detectable outside of the methane-absorbing region, then it must be a false positive. However, for hyperspectral data, we have not experimentally validated the hypothesis that providing the additional bands results in better performance.","title":"Which of the spectral bands do you use?"},{"location":"Hyperspectral.html#have-you-tried-training-a-hyperspectral-model-with-reference-scenes","text":"For a number of reasons, we have not tried training a hyperspectral model with reference scenes. EMIT has a fine spectral resolution making it well suited to the problem of methane detection, whereas Sentinel-2 and Landsat 8/9 each have two broad bands covering methane-absorbing spectra. This suggests that a machine learning approach using a single acquisition is possible for EMIT while historical reference imagery is necessary to draw out the methane signal for the other satellites. Additionally, EMIT has sparse historical coverage compared to the other satellites, meaning that high quality reference scenes may not be available for a given location or they are likely to be further removed in time. Finally, poor reference scenes can frustrate inference; the presence of unmasked clouds or an emissions event in reference imagery can cause incorrect predictions, particularly false negatives. Given this, we decided to approach the methane detection problem using a single scene (monotemporal imagery) in the case of EMIT rather than try to leverage reference imagery","title":"Have you tried training a hyperspectral model with reference scenes?"},{"location":"Hyperspectral.html#results-and-limitations","text":"The EMIT CV model achieved the following scores on the validation set: F1 score: 0.720 Recall: 0.584 Precision: 0.938 Mean squared error: 0.099 Given that the validation set had been created by synthetically inserting plumes, we put particular weight on qualitatively evaluating an additional set of 61 known emissions identified by IMEO in their original granules, i.e. without having been synthetically inserted. As a comparison baseline we used the matched filter (MF) algorithm implementation mag1c by Foote et al . The EMIT CV model successfully detected the vast majority of these emissions events. False negatives generally correlated with at least one of the following criteria: difficult image background, low methane concentrations, or, particularly, spatially small or sparse methane-containing pixels. An example of a true positive and a false negative are shown below. Figure: Example of a true positive detection by the EMIT CV model. Figure: Example of a false negative detection by the EMIT CV model. The finding that the model misses a number of small or sparse plumes suggests that changes to the loss function could allow for an improved model. The mean squared error portion of the model is relatively insensitive to errors of a few pixels. For larger plumes this makes sense: missing only a few pixels would often be considered good enough. However, entirely missing a small plume of a few pixels is more problematic from an operational perspective given that recall is of utmost importance, especially given that at the EMIT resolution of 60 meters, small plumes will be a regular occurrence. Tweaking the loss function to account for this could be promising but has not yet been tested.","title":"Results and limitations"},{"location":"Hyperspectral.html#does-the-emit-cv-model-outperform-the-matched-filter","text":"It would be difficult to assert that one method is outright better than the other. From the evaluation of IMEO notified plumes, the matched filter does identify methane in a number of cases where the EMIT CV model either predicts with very low confidence or misses the methane altogether. As discussed above, these are generally cases of very small or sparse plumes. The matched filter results contain considerable noise, which we have frequently found to frustrate the automatic segmentation of plumes. The EMIT CV model produces results with a much higher signal-to-noise ratio, making them easier to segment automatically. Given the advantages of each approach, the matched filter is still better suited to cases where manual review and segmentation are feasible. This will be over smaller amounts of imagery. For large-scale processing and automatic plume segmentation, the clean outputs of the EMIT CV model are preferable even if this will result in lower recall.","title":"Does the EMIT CV model outperform the matched filter?"},{"location":"Hyperspectral.html#references","text":"Foote, M. D., et al. \"Fast and Accurate Retrieval of Methane Concentration from Imaging Spectrometer Data Using Sparsity Prior\" IEEE Transactions on Geoscience and Remote Sensing. 2020. R\u016f\u017ei\u010dka, V\u00edt, and Andrew Markham. \"HyperspectralViTs: General Hyperspectral Models for On-board Remote Sensing.\" IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (2025).","title":"References"},{"location":"Landsat.html","text":"Landsat Can you apply the Sentinel-2 model without retraining on Landsat data? Yes \u2014 we\u2019ve experimented with applying our Sentinel-2 model directly to Landsat imagery using the same simulated plume insertion and frac prediction approach . However, the results were suboptimal. While it technically works, the performance drops due to differences in resolution, sensor characteristics, and spectral coverage between Sentinel-2 and Landsat. To address this, we trained a dedicated model on Landsat data using the same methodology. This led to significantly better performance. What are the results of your best Landsat model? Below are the detection threshold results for the Landsat model (corresponding to an average FPR of around 1.3 plumes per 352\u00d7352 tile\u2014area-equivalent to 500\u00d7500 px tiles in Sentinel-2): Hassi (Algeria) We detect emissions of 177 kg/hr in 10% of cases 50% DT: 481 kg/hr 90% DT: 2287 kg/hr Permian (USA) 10% DT: 552 kg/hr 50% DT: 1835 kg/hr 90% DT: 10,350 kg/hr Marcellus (USA) 10% DT: 2064 kg/hr 50% DT: 8583 kg/hr 90% DT: >25000 kg/hr Interestingly, the Landsat model achieves better 10% and 50% detection thresholds than our current Sentinel-2 model , though 90% thresholds are worse \u2014likely due to differences in signal-to-noise characteristics and scene variability. This result highlights the value of training sensor-specific models, rather than relying on direct transfer from one platform to another. What are the differences between the approach used for Sentinel-2 and Landsat? The main difference lies in how the data is generated and ingested. Landsat and Sentinel-2 differ in spatial resolution, tiling format, and metadata structure, which impacts how we process and align scenes for methane detection. Sensor Coverage and Temporal Depth We use only Landsat 8 and 9 because they are the two active missions still collecting data, which makes them suitable for real-time methane monitoring. Landsat 8 also gives us a longer history, with data available since 2013\u2014earlier than Sentinel-2, which started in 2016. Spatial Resolution Sentinel-2 has 20m resolution, Landsat has 30m (in the relevant methane absorbing bands). Tiling Alignment Sentinel-2 is naturally organized in fixed MGRS tiles (100\u202fkm \u00d7 100\u202fkm), which makes spatial alignment straightforward and consistent across time. Landsat images are delivered as WRS-2 path/row scenes , which are not aligned to a fixed grid. This introduces spatial variability across acquisitions and makes it harder to stack and compare scenes directly. To address this, we implemented a fixed internal tile grid for Landsat (e.g. 384\u00d7384 pixel tiles) and reprojected each Landsat acquisition to this grid, enabling consistent spatial alignment across time and scenes. Reference Scenes Alignment Creating a time series stack of reference scenes for Sentinel-2 is straightforward\u2014 the scenes are already orthorectified, snapped to a global UTM-based grid (MGRS), and aligned at the pixel level. Each scene is delivered in a consistent spatial reference system, so time steps are co-registered by default. That means we can compare across dates directly, without worrying about reprojection artifacts or pixel shifts. Landsat, on the other hand, follows the WRS-2 path/row system. Each scene covers a full swath and isn\u2019t aligned to a consistent global tiling grid. Even though the data is orthorectified, there\u2019s no guarantee that pixels line up across dates. So for Landsat, we have to reproject all reference scenes to the grid of the main (target) scene to do a time series analysis. Cloud Masking For Sentinel-2, we use an external model ( OmniCloud ) to generate high-quality cloud masks. For Landsat, we found the native QA_PIXEL band sufficient to exclude invalid pixels (cloud, shadow, etc).","title":"Landsat"},{"location":"Landsat.html#landsat","text":"","title":"Landsat"},{"location":"Landsat.html#can-you-apply-the-sentinel-2-model-without-retraining-on-landsat-data","text":"Yes \u2014 we\u2019ve experimented with applying our Sentinel-2 model directly to Landsat imagery using the same simulated plume insertion and frac prediction approach . However, the results were suboptimal. While it technically works, the performance drops due to differences in resolution, sensor characteristics, and spectral coverage between Sentinel-2 and Landsat. To address this, we trained a dedicated model on Landsat data using the same methodology. This led to significantly better performance.","title":"Can you apply the Sentinel-2 model without retraining on Landsat data?"},{"location":"Landsat.html#what-are-the-results-of-your-best-landsat-model","text":"Below are the detection threshold results for the Landsat model (corresponding to an average FPR of around 1.3 plumes per 352\u00d7352 tile\u2014area-equivalent to 500\u00d7500 px tiles in Sentinel-2): Hassi (Algeria) We detect emissions of 177 kg/hr in 10% of cases 50% DT: 481 kg/hr 90% DT: 2287 kg/hr Permian (USA) 10% DT: 552 kg/hr 50% DT: 1835 kg/hr 90% DT: 10,350 kg/hr Marcellus (USA) 10% DT: 2064 kg/hr 50% DT: 8583 kg/hr 90% DT: >25000 kg/hr Interestingly, the Landsat model achieves better 10% and 50% detection thresholds than our current Sentinel-2 model , though 90% thresholds are worse \u2014likely due to differences in signal-to-noise characteristics and scene variability. This result highlights the value of training sensor-specific models, rather than relying on direct transfer from one platform to another.","title":"What are the results of your best Landsat model?"},{"location":"Landsat.html#what-are-the-differences-between-the-approach-used-for-sentinel-2-and-landsat","text":"The main difference lies in how the data is generated and ingested. Landsat and Sentinel-2 differ in spatial resolution, tiling format, and metadata structure, which impacts how we process and align scenes for methane detection.","title":"What are the differences between the approach used for Sentinel-2 and Landsat?"},{"location":"Landsat.html#sensor-coverage-and-temporal-depth","text":"We use only Landsat 8 and 9 because they are the two active missions still collecting data, which makes them suitable for real-time methane monitoring. Landsat 8 also gives us a longer history, with data available since 2013\u2014earlier than Sentinel-2, which started in 2016.","title":"Sensor Coverage and Temporal Depth"},{"location":"Landsat.html#spatial-resolution","text":"Sentinel-2 has 20m resolution, Landsat has 30m (in the relevant methane absorbing bands).","title":"Spatial Resolution"},{"location":"Landsat.html#tiling-alignment","text":"Sentinel-2 is naturally organized in fixed MGRS tiles (100\u202fkm \u00d7 100\u202fkm), which makes spatial alignment straightforward and consistent across time. Landsat images are delivered as WRS-2 path/row scenes , which are not aligned to a fixed grid. This introduces spatial variability across acquisitions and makes it harder to stack and compare scenes directly. To address this, we implemented a fixed internal tile grid for Landsat (e.g. 384\u00d7384 pixel tiles) and reprojected each Landsat acquisition to this grid, enabling consistent spatial alignment across time and scenes.","title":"Tiling Alignment"},{"location":"Landsat.html#reference-scenes-alignment","text":"Creating a time series stack of reference scenes for Sentinel-2 is straightforward\u2014 the scenes are already orthorectified, snapped to a global UTM-based grid (MGRS), and aligned at the pixel level. Each scene is delivered in a consistent spatial reference system, so time steps are co-registered by default. That means we can compare across dates directly, without worrying about reprojection artifacts or pixel shifts. Landsat, on the other hand, follows the WRS-2 path/row system. Each scene covers a full swath and isn\u2019t aligned to a consistent global tiling grid. Even though the data is orthorectified, there\u2019s no guarantee that pixels line up across dates. So for Landsat, we have to reproject all reference scenes to the grid of the main (target) scene to do a time series analysis.","title":"Reference Scenes Alignment"},{"location":"Landsat.html#cloud-masking","text":"For Sentinel-2, we use an external model ( OmniCloud ) to generate high-quality cloud masks. For Landsat, we found the native QA_PIXEL band sufficient to exclude invalid pixels (cloud, shadow, etc).","title":"Cloud Masking"},{"location":"Landsat.html#_1","text":"","title":""},{"location":"Model.html","text":"Model What is the loss function used for training? We use a custom loss function, which is a weighted sum of a binary cross-entropy and a mean squared error loss function. Each loss function is applied to a separate output channel of the neural network, so in that sense our methane detection model is a multi-task model. The binary cross-entropy is obtained by thresholding the labels \u2013 pixels with a fractional reduction y \u1d62 in the B12/B11 ratio that exceed the binary_threshold hyperparameter (set to 0.001) are encoded as 1's, and those below as 0's. \\[ b_i = \\begin{cases} 1 & \\text{if }|y_i| > T_{binary} \\\\ 0 & \\text{if } |y_i| \\le T_{binary} \\end{cases} \\] \\[ \\mathcal{L}_{BCE} = - \\sum_{i=1}^{N} \\left[ b_i \\log(\\hat{p}_i) + (1 - b_i) \\log(1 - \\hat{p}_i) \\right] \\] The mean squared error loss is calculated only on pixels that exceed the binary_threshold hyperparameter, and it is weighted by the MSE_multiplier hyperparameter. \\[ \\mathcal{L}_{MSE} = \\sum_{i=1}^{N} b_i (y_i - \\hat{\\mu}_i)^2 \\] \\[ \\mathcal{L} = \\mathcal{L}_{BCE} +W_{MSE} \\cdot \\mathcal{L}_{MSE} \\] How should the model output be interpreted? The U-net outputs two channels that together give a probabilistic prediction of methane. The first is a segmentation channel which gives a binary prediction of the presence or absence of methane in each pixel. Presence is defined as the fractional reduction in the B12/B11 band ratio exceeding the binary_threshold value (0.001). Pass this channel through a sigmoid (inverse logistic) function to obtain a probability between 0 and 1. This is the model's internal predictive probability of methane, but it should be used with caution, as it is calibrated on the training data, which contains many more methane plumes than would be encountered \"in the wild\". Changing the chip size (from 128x128 to 256x256) or the number or sizes of plumes inserted in each chip results in different output probabilities. The second is a regression channel which gives a conditional prediction of the quantity of methane present in each pixel. In other words, it answers the question \"if there is methane in this pixel, then how much?\" The metric is the fractional change in the B12/B11 band ratio (frac), and still needs to be post-processed to obtain a gas concentration in mol/m\u00b2. The loss function on the conditional channel only applies to pixels that contain methane, so outside of these areas, the model is free to conjure up possible methane plumes \"just in case.\" The conditional channel on its own therefore tends to be noisy. A \"marginal\" prediction can be obtained by multiplying the binary probability and the conditional prediction. This can be used like the output of a pixelwise regression model, as a point prediction of the quantity of methane in each pixel. But again, it is calibrated to the training data, and so should be interpreted cautiously. A probabilistic interpretation that combines the two predictive channels is that the prediction for each pixel is modelled as a zero-inflated Gaussian distribution. The loss function is the likelihood of this distribution, and the optimizer's task is to maximize this likelihood. In this perspective, the MSE_multiplier parameter controls the \u03c3 parameter of the predictive Gaussian distribution. It is inversely proportional to the variance. Since frac values are small, informally this explains why MSE_multiplier needs to be high for the two loss components to be in tune with each other. The marginal prediction is the expected value of the zero-inflated distribution. Why isn't the model just a semantic segmentation? Why isn't the model just a pixelwise regression? A semantic segmentation model (pixelwise binary classification) is insufficient to be able to report methane emissions from assets, as it only indicates the presence of a plume, but not its concentration. In order to quantify the emissions, one would have to use the segmentation output as a mask for a separately retrieved methane concentration map. This can be a successful strategy, for example Rouet-Leduc and Hulbert (2024) train a pixel classifier which they use to mask the Varon et al. (2021) \"classical\" retrievals, and thus reduce false positives. We did not follow this strategy, as we found the classical retrievals to be noisy, sometimes even reporting negative methane concentrations on pixels classified as methane. We chose to leverage the power of neural networks for high-quality retrievals as well as masking. A pixelwise regression model (where the neural network predicts the quantity of methane or frac in each pixel) could also be trained for this problem. The main issue here is that the ground truth is very sparse (most pixels don't contain methane), and so with a mean squared error loss most predictions are strongly regularized towards zero. Sometimes during model training the weights would even collapse to always predicting zero everywhere. We could only counter-balance this somewhat by giving higher weight to methane pixels, but this introduced an additional hyperparameter that is difficult to tune. We therefore chose the multi-task approach (two-part loss) which gives the best of both worlds \u2013 separating mask and retrieval outputs with a neat probabilistic interpretation, and adapting well to the sparseness of the ground truth data. Is class imbalance a problem? Though we often worried about class imbalance (most pixels do not contain methane), we found that the binary cross-entropy loss handled it well, and the trained models did not simply predict no methane everywhere. Experiments with using focal loss \u2013 which is designed to cope better with class imbalance \u2013 yielded much worse results. Does the model ever infer negative methane concentrations? Yes, this can happen, as the conditional prediction is unconstrained. Some (but not all) trained models predict negative methane in areas where it is quite sure there is no methane. Even though this is physically impossible, there is no penalty in the loss function for outputting a negative prediction on methane-free pixels. If desired, it would be quite simple to constrain the model to only predict positive methane, by passing the conditional prediction layer through a sigmoid function before applying the masked mean squared error function. We have not tried this, as we found the unconstrained conditional predictions to generally behave well: we do not see significant negative concentrations in the marginal predictions. What model architecture are you using? We are using an U-Net++ (Zhou et al. 2018) with an efficientnet-b4 (Tan, Le, 2019) encoder starting with \u201cnoisy-student\u201d pretrained weights. Using the Segmentation Models Python package this model can be initialized with import segmentation_models_pytorch as smp model = smp.UnetPlusPlus( encoder_name=\"timm-efficientnet-b4\", encoder_weights=\"noisy-student\", in_channels=..., classes=... ) Have you tried any other model architectures? We saw a large improvement going from an U-Net (Ronneberger et al. 2015) with a ResNet-50 (He et al. 2016) encoder to an U-Net++ (Zhou et al. 2018) with an efficientnet-b4 (Tan, Le, 2019). Larger encoders, i.e. going from efficientnet-b1 to b2 to b3 to b4 gave us small, but significant improvements. How do you train the model? The training loop (code here ) is fairly standard with some special design decisions. We typically trained with 4 GPUs using distributed training on Azure ML. We use the optimizer AdamW with no weight decay and a batch size schedule (Smith et al. 2017), where the batch size is gradually increased over the first 30 warmup epochs from 16 to 160. Unlike most computer vision applications, because we can generate arbitrarily large synthetic datasets, using augmentations to juice as much information as possible out of every sample is not essential. So we only use simple non-destructive augmentations: random rotations by multiples of 90\u00b0, and random flips. In earlier model training, we also implemented and used a signal modulation transformation that weakens the methane concentration by a random constant factor in each chip. We found that presenting the model with stronger plumes in early epochs, and gradually reducing them in later epochs, helped guide the optimizer. In more recent training runs, we noticed that the modulation was no longer needed to get good results. Validation metrics are calculated on three pre-selected areas (see Why have you selected these three specific regions to validate in? ) and training is only stopped if the validation metrics are not improving for multiple epochs (Early Stopping with patience of X). As the main validation metric for Early Stopping, we selected average Recall over multiple emission buckets and averaged over the three regions. Recall is calculated using a classification threshold that allows for 10 false positive pixels on average over all 128x128 validation crops. A model checkpoint is saved if the mean recall improves. Balancing False Positives and Recall allows us to select the model that offers the highest sensitivity (i.e. best detection accuracy) without exceeding our target noise tolerance. See here for the implementation of validation and here for more details and validation metric visuals.. How long does it take to train the model? Around 15 hours depending on some randomness in the Early Stopping criteria. We have been using the Azure VM Standard_NC64as_T4_v3 (64 cores, 440 GB RAM, 2816 GB disk) with 4 x NVIDIA Tesla T4 GPUs. Our typical throughput is in the 550-600 chips/second (a chip is a single data item) range during training, and over 2,000 chips/second during validation (where gradients do not need to be computed). By modern standards, these are not particularly powerful GPUs, but we find that because of the large number of bands, more powerful GPUs would not be used efficiently and the training quickly becomes IO-bound. Have you tried using foundation models? Yes, we ran some experiments with pretrained weights from popular foundation models trained on generic computer vision tasks for Sentinel 2 data. We did not see any measurable improvement in model performance from those. Our informal interpretation is that the methane signal in bands 11 and 12 is very subtle, and its detection quite orthogonal to more typical computer vision tasks on satellite imagery, such as object detection, change detection and land cover classification. What experiments did not work? We have found the performance to be not affected by the exact number of synthetic plumes we are inserting into training chips. Our default was to insert a random number of between 0-5 plumes into each chip, but we have seen similar results using 0-2, 0-3 and 0-4. Only 0-1 showed significantly worse results as now we seem to show too little plumes. Also, at our current dataset size of around 1.5 million 128x128 chips, we have found the dataset size to not have an impact anymore on performance. There are large gains from going from 0.1 million to 0.75 million, but doubling the size again to 1.5 million yields no significant gains. We tried using larger chips, going from 128x128 to 256x256 following the intuition that having more context and less border pixels could help the model in learning better. To our surprise, we got worse results. Also, using the same 256x256 chips, we tried using random crops of 224x224, 192x192 and 160x160 as another data augmentation technique. This did not work as well, the best results were achieved giving the full size chips. What are some of the main limitations of these models? Despite significant strides in model performance over the last year, the primary limitation of the Sentinel 2 computer vision model remains false positives. Large methane plumes detectable by Sentinel 2 are relatively rare events, and so even a small rate of false positives can easily overwhelm the true detections. For this reason, we find that we continue to need careful quality assurance (QA) of candidate plumes in production. In recent models trained on simulated plumes, we observed that the model would sometimes create false positives with very realistic-looking plume morphologies. Our interpretation is that a model that leverages morphological information also risks being prone to hallucinating more realistic plumes. These are therefore difficult to invalidate in QA. For this reason, we do not use these models in our production setting. Just like the classical retrieval algorithms, our computer vision model relies heavily on reference scenes to infer the presence of methane on a target date. This works well when a scene is stable (such as in deserts), but breaks down if a scene changes rapidly (vegetation, agriculture, or large changes in soil moisture). In the classical algorithms, such changes cause false positives, whereas with computer vision they tend to raise the detection threshold. Another subtle consequence is that persistent sources can sometimes be erased if methane is present in one of the reference scenes, especially if the wind direction does not change between overpasses. We observed this in single blind release studies, where we found we needed to manually select alternative reference scenes to recover certain plumes. What are the most important next steps? Reference chip selection Our current method to select two reference scenes only uses a cloud masking threshold and proximity in time. When we examine scenes with known methane emissions, we find that plumes that are missed (false negatives) by the computer vision inference can be revealed with a manual selection of alternative reference scenes. This can happen if the source also emitted methane during one of the reference overpasses, which erases the methane signal on the target date. It can also happen if scenes further in the past are better references than more recent ones, for example in terms of soil moisture conditions. We therefore expect this to be fertile ground for model improvements, including ideas such as: feature engineering reference scenes, like taking averages of the last 5/10 overpasses; selecting the reference scenes by some similarity metric to the target date; allowing the model to choose its own reference scenes by some attention mechanism. Hyperparameter tuning for simulated plumes There are many decisions that need to be made when it comes to synthetic data generation with simulated plumes, including multiple hyperparameters that we have observed to be important to model performance, but have not yet systematically tuned. These are: the distributions of emission sizes in the training data, the diffusion and turbulence parameters of the Gaussian plume models the distribution of the number of plumes to include in each chip (we currently sample uniformly from 0 to 5 for each chip) The development cycle for these parameters is long, as it includes plume simulations, synthetic data generation, model training and validation. But there may well significant detection performance improvements available if pursued. References He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778). Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" In Medical image computing and computer-assisted intervention\u2013MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18 , pp. 234-241. Springer international publishing, 2015. Rouet-Leduc, Bertrand, and Claudia Hulbert. \"Automatic detection of methane emissions in multispectral satellite imagery using a vision transformer.\" Nature Communications 15, no. 1 (2024): 3801. Smith, S. L., Kindermans, P. J., Ying, C., & Le, Q. V. (2017). Don't decay the learning rate, increase the batch size. arXiv preprint arXiv:1711.00489 . Tan, M., & Le, Q. (2019, May). Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning (pp. 6105-6114). PMLR. Varon, Daniel J., Dylan Jervis, Jason McKeever, Ian Spence, David Gains, and Daniel J. Jacob. \"High-frequency monitoring of anomalous methane point sources with multispectral Sentinel-2 satellite observations.\" Atmospheric Measurement Techniques 14, no. 4 (2021). Zhou, Z., Rahman Siddiquee, M. M., Tajbakhsh, N., & Liang, J. (2018). Unet++: A nested u-net architecture for medical image segmentation. In Deep learning in medical image analysis and multimodal learning for clinical decision support: 4th international workshop, DLMIA 2018, and 8th international workshop, ML-CDS 2018, held in conjunction with MICCAI 2018, Granada, Spain, September 20, 2018, proceedings 4 (pp. 3-11) . Springer International Publishing.","title":"Model"},{"location":"Model.html#model","text":"","title":"Model"},{"location":"Model.html#what-is-the-loss-function-used-for-training","text":"We use a custom loss function, which is a weighted sum of a binary cross-entropy and a mean squared error loss function. Each loss function is applied to a separate output channel of the neural network, so in that sense our methane detection model is a multi-task model. The binary cross-entropy is obtained by thresholding the labels \u2013 pixels with a fractional reduction y \u1d62 in the B12/B11 ratio that exceed the binary_threshold hyperparameter (set to 0.001) are encoded as 1's, and those below as 0's. \\[ b_i = \\begin{cases} 1 & \\text{if }|y_i| > T_{binary} \\\\ 0 & \\text{if } |y_i| \\le T_{binary} \\end{cases} \\] \\[ \\mathcal{L}_{BCE} = - \\sum_{i=1}^{N} \\left[ b_i \\log(\\hat{p}_i) + (1 - b_i) \\log(1 - \\hat{p}_i) \\right] \\] The mean squared error loss is calculated only on pixels that exceed the binary_threshold hyperparameter, and it is weighted by the MSE_multiplier hyperparameter. \\[ \\mathcal{L}_{MSE} = \\sum_{i=1}^{N} b_i (y_i - \\hat{\\mu}_i)^2 \\] \\[ \\mathcal{L} = \\mathcal{L}_{BCE} +W_{MSE} \\cdot \\mathcal{L}_{MSE} \\]","title":"What is the loss function used for training?"},{"location":"Model.html#how-should-the-model-output-be-interpreted","text":"The U-net outputs two channels that together give a probabilistic prediction of methane. The first is a segmentation channel which gives a binary prediction of the presence or absence of methane in each pixel. Presence is defined as the fractional reduction in the B12/B11 band ratio exceeding the binary_threshold value (0.001). Pass this channel through a sigmoid (inverse logistic) function to obtain a probability between 0 and 1. This is the model's internal predictive probability of methane, but it should be used with caution, as it is calibrated on the training data, which contains many more methane plumes than would be encountered \"in the wild\". Changing the chip size (from 128x128 to 256x256) or the number or sizes of plumes inserted in each chip results in different output probabilities. The second is a regression channel which gives a conditional prediction of the quantity of methane present in each pixel. In other words, it answers the question \"if there is methane in this pixel, then how much?\" The metric is the fractional change in the B12/B11 band ratio (frac), and still needs to be post-processed to obtain a gas concentration in mol/m\u00b2. The loss function on the conditional channel only applies to pixels that contain methane, so outside of these areas, the model is free to conjure up possible methane plumes \"just in case.\" The conditional channel on its own therefore tends to be noisy. A \"marginal\" prediction can be obtained by multiplying the binary probability and the conditional prediction. This can be used like the output of a pixelwise regression model, as a point prediction of the quantity of methane in each pixel. But again, it is calibrated to the training data, and so should be interpreted cautiously. A probabilistic interpretation that combines the two predictive channels is that the prediction for each pixel is modelled as a zero-inflated Gaussian distribution. The loss function is the likelihood of this distribution, and the optimizer's task is to maximize this likelihood. In this perspective, the MSE_multiplier parameter controls the \u03c3 parameter of the predictive Gaussian distribution. It is inversely proportional to the variance. Since frac values are small, informally this explains why MSE_multiplier needs to be high for the two loss components to be in tune with each other. The marginal prediction is the expected value of the zero-inflated distribution.","title":"How should the model output be interpreted?"},{"location":"Model.html#why-isnt-the-model-just-a-semantic-segmentation-why-isnt-the-model-just-a-pixelwise-regression","text":"A semantic segmentation model (pixelwise binary classification) is insufficient to be able to report methane emissions from assets, as it only indicates the presence of a plume, but not its concentration. In order to quantify the emissions, one would have to use the segmentation output as a mask for a separately retrieved methane concentration map. This can be a successful strategy, for example Rouet-Leduc and Hulbert (2024) train a pixel classifier which they use to mask the Varon et al. (2021) \"classical\" retrievals, and thus reduce false positives. We did not follow this strategy, as we found the classical retrievals to be noisy, sometimes even reporting negative methane concentrations on pixels classified as methane. We chose to leverage the power of neural networks for high-quality retrievals as well as masking. A pixelwise regression model (where the neural network predicts the quantity of methane or frac in each pixel) could also be trained for this problem. The main issue here is that the ground truth is very sparse (most pixels don't contain methane), and so with a mean squared error loss most predictions are strongly regularized towards zero. Sometimes during model training the weights would even collapse to always predicting zero everywhere. We could only counter-balance this somewhat by giving higher weight to methane pixels, but this introduced an additional hyperparameter that is difficult to tune. We therefore chose the multi-task approach (two-part loss) which gives the best of both worlds \u2013 separating mask and retrieval outputs with a neat probabilistic interpretation, and adapting well to the sparseness of the ground truth data.","title":"Why isn't the model just a semantic segmentation? Why isn't the model just a pixelwise regression?"},{"location":"Model.html#is-class-imbalance-a-problem","text":"Though we often worried about class imbalance (most pixels do not contain methane), we found that the binary cross-entropy loss handled it well, and the trained models did not simply predict no methane everywhere. Experiments with using focal loss \u2013 which is designed to cope better with class imbalance \u2013 yielded much worse results.","title":"Is class imbalance a problem?"},{"location":"Model.html#does-the-model-ever-infer-negative-methane-concentrations","text":"Yes, this can happen, as the conditional prediction is unconstrained. Some (but not all) trained models predict negative methane in areas where it is quite sure there is no methane. Even though this is physically impossible, there is no penalty in the loss function for outputting a negative prediction on methane-free pixels. If desired, it would be quite simple to constrain the model to only predict positive methane, by passing the conditional prediction layer through a sigmoid function before applying the masked mean squared error function. We have not tried this, as we found the unconstrained conditional predictions to generally behave well: we do not see significant negative concentrations in the marginal predictions.","title":"Does the model ever infer negative methane concentrations?"},{"location":"Model.html#what-model-architecture-are-you-using","text":"We are using an U-Net++ (Zhou et al. 2018) with an efficientnet-b4 (Tan, Le, 2019) encoder starting with \u201cnoisy-student\u201d pretrained weights. Using the Segmentation Models Python package this model can be initialized with import segmentation_models_pytorch as smp model = smp.UnetPlusPlus( encoder_name=\"timm-efficientnet-b4\", encoder_weights=\"noisy-student\", in_channels=..., classes=... )","title":"What model architecture are you using?"},{"location":"Model.html#have-you-tried-any-other-model-architectures","text":"We saw a large improvement going from an U-Net (Ronneberger et al. 2015) with a ResNet-50 (He et al. 2016) encoder to an U-Net++ (Zhou et al. 2018) with an efficientnet-b4 (Tan, Le, 2019). Larger encoders, i.e. going from efficientnet-b1 to b2 to b3 to b4 gave us small, but significant improvements.","title":"Have you tried any other model architectures?"},{"location":"Model.html#how-do-you-train-the-model","text":"The training loop (code here ) is fairly standard with some special design decisions. We typically trained with 4 GPUs using distributed training on Azure ML. We use the optimizer AdamW with no weight decay and a batch size schedule (Smith et al. 2017), where the batch size is gradually increased over the first 30 warmup epochs from 16 to 160. Unlike most computer vision applications, because we can generate arbitrarily large synthetic datasets, using augmentations to juice as much information as possible out of every sample is not essential. So we only use simple non-destructive augmentations: random rotations by multiples of 90\u00b0, and random flips. In earlier model training, we also implemented and used a signal modulation transformation that weakens the methane concentration by a random constant factor in each chip. We found that presenting the model with stronger plumes in early epochs, and gradually reducing them in later epochs, helped guide the optimizer. In more recent training runs, we noticed that the modulation was no longer needed to get good results. Validation metrics are calculated on three pre-selected areas (see Why have you selected these three specific regions to validate in? ) and training is only stopped if the validation metrics are not improving for multiple epochs (Early Stopping with patience of X). As the main validation metric for Early Stopping, we selected average Recall over multiple emission buckets and averaged over the three regions. Recall is calculated using a classification threshold that allows for 10 false positive pixels on average over all 128x128 validation crops. A model checkpoint is saved if the mean recall improves. Balancing False Positives and Recall allows us to select the model that offers the highest sensitivity (i.e. best detection accuracy) without exceeding our target noise tolerance. See here for the implementation of validation and here for more details and validation metric visuals..","title":"How do you train the model?"},{"location":"Model.html#how-long-does-it-take-to-train-the-model","text":"Around 15 hours depending on some randomness in the Early Stopping criteria. We have been using the Azure VM Standard_NC64as_T4_v3 (64 cores, 440 GB RAM, 2816 GB disk) with 4 x NVIDIA Tesla T4 GPUs. Our typical throughput is in the 550-600 chips/second (a chip is a single data item) range during training, and over 2,000 chips/second during validation (where gradients do not need to be computed). By modern standards, these are not particularly powerful GPUs, but we find that because of the large number of bands, more powerful GPUs would not be used efficiently and the training quickly becomes IO-bound.","title":"How long does it take to train the model?"},{"location":"Model.html#have-you-tried-using-foundation-models","text":"Yes, we ran some experiments with pretrained weights from popular foundation models trained on generic computer vision tasks for Sentinel 2 data. We did not see any measurable improvement in model performance from those. Our informal interpretation is that the methane signal in bands 11 and 12 is very subtle, and its detection quite orthogonal to more typical computer vision tasks on satellite imagery, such as object detection, change detection and land cover classification.","title":"Have you tried using foundation models?"},{"location":"Model.html#what-experiments-did-not-work","text":"We have found the performance to be not affected by the exact number of synthetic plumes we are inserting into training chips. Our default was to insert a random number of between 0-5 plumes into each chip, but we have seen similar results using 0-2, 0-3 and 0-4. Only 0-1 showed significantly worse results as now we seem to show too little plumes. Also, at our current dataset size of around 1.5 million 128x128 chips, we have found the dataset size to not have an impact anymore on performance. There are large gains from going from 0.1 million to 0.75 million, but doubling the size again to 1.5 million yields no significant gains. We tried using larger chips, going from 128x128 to 256x256 following the intuition that having more context and less border pixels could help the model in learning better. To our surprise, we got worse results. Also, using the same 256x256 chips, we tried using random crops of 224x224, 192x192 and 160x160 as another data augmentation technique. This did not work as well, the best results were achieved giving the full size chips.","title":"What experiments did not work?"},{"location":"Model.html#what-are-some-of-the-main-limitations-of-these-models","text":"Despite significant strides in model performance over the last year, the primary limitation of the Sentinel 2 computer vision model remains false positives. Large methane plumes detectable by Sentinel 2 are relatively rare events, and so even a small rate of false positives can easily overwhelm the true detections. For this reason, we find that we continue to need careful quality assurance (QA) of candidate plumes in production. In recent models trained on simulated plumes, we observed that the model would sometimes create false positives with very realistic-looking plume morphologies. Our interpretation is that a model that leverages morphological information also risks being prone to hallucinating more realistic plumes. These are therefore difficult to invalidate in QA. For this reason, we do not use these models in our production setting. Just like the classical retrieval algorithms, our computer vision model relies heavily on reference scenes to infer the presence of methane on a target date. This works well when a scene is stable (such as in deserts), but breaks down if a scene changes rapidly (vegetation, agriculture, or large changes in soil moisture). In the classical algorithms, such changes cause false positives, whereas with computer vision they tend to raise the detection threshold. Another subtle consequence is that persistent sources can sometimes be erased if methane is present in one of the reference scenes, especially if the wind direction does not change between overpasses. We observed this in single blind release studies, where we found we needed to manually select alternative reference scenes to recover certain plumes.","title":"What are some of the main limitations of these models?"},{"location":"Model.html#what-are-the-most-important-next-steps","text":"","title":"What are the most important next steps?"},{"location":"Model.html#reference-chip-selection","text":"Our current method to select two reference scenes only uses a cloud masking threshold and proximity in time. When we examine scenes with known methane emissions, we find that plumes that are missed (false negatives) by the computer vision inference can be revealed with a manual selection of alternative reference scenes. This can happen if the source also emitted methane during one of the reference overpasses, which erases the methane signal on the target date. It can also happen if scenes further in the past are better references than more recent ones, for example in terms of soil moisture conditions. We therefore expect this to be fertile ground for model improvements, including ideas such as: feature engineering reference scenes, like taking averages of the last 5/10 overpasses; selecting the reference scenes by some similarity metric to the target date; allowing the model to choose its own reference scenes by some attention mechanism.","title":"Reference chip selection"},{"location":"Model.html#hyperparameter-tuning-for-simulated-plumes","text":"There are many decisions that need to be made when it comes to synthetic data generation with simulated plumes, including multiple hyperparameters that we have observed to be important to model performance, but have not yet systematically tuned. These are: the distributions of emission sizes in the training data, the diffusion and turbulence parameters of the Gaussian plume models the distribution of the number of plumes to include in each chip (we currently sample uniformly from 0 to 5 for each chip) The development cycle for these parameters is long, as it includes plume simulations, synthetic data generation, model training and validation. But there may well significant detection performance improvements available if pursued.","title":"Hyperparameter tuning for simulated plumes"},{"location":"Model.html#references","text":"He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778). Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" In Medical image computing and computer-assisted intervention\u2013MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18 , pp. 234-241. Springer international publishing, 2015. Rouet-Leduc, Bertrand, and Claudia Hulbert. \"Automatic detection of methane emissions in multispectral satellite imagery using a vision transformer.\" Nature Communications 15, no. 1 (2024): 3801. Smith, S. L., Kindermans, P. J., Ying, C., & Le, Q. V. (2017). Don't decay the learning rate, increase the batch size. arXiv preprint arXiv:1711.00489 . Tan, M., & Le, Q. (2019, May). Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning (pp. 6105-6114). PMLR. Varon, Daniel J., Dylan Jervis, Jason McKeever, Ian Spence, David Gains, and Daniel J. Jacob. \"High-frequency monitoring of anomalous methane point sources with multispectral Sentinel-2 satellite observations.\" Atmospheric Measurement Techniques 14, no. 4 (2021). Zhou, Z., Rahman Siddiquee, M. M., Tajbakhsh, N., & Liang, J. (2018). Unet++: A nested u-net architecture for medical image segmentation. In Deep learning in medical image analysis and multimodal learning for clinical decision support: 4th international workshop, DLMIA 2018, and 8th international workshop, ML-CDS 2018, held in conjunction with MICCAI 2018, Granada, Spain, September 20, 2018, proceedings 4 (pp. 3-11) . Springer International Publishing.","title":"References"},{"location":"Model.html#_1","text":"","title":""},{"location":"Postprocessing.html","text":"Postprocessing How are individual plumes masked (segmented) out of the inference output? The masking stage in the methane detection pipeline is designed to convert sensor outputs into binary masks that isolate methane plume signals. Our masking is performed on a binary probability map that represents the likelihood of methane presence at each pixel. We use a watershed segmentation algorithm, a classical image processing technique that treats the probability map as a topographic surface. High-probability pixels are treated as peaks, and the algorithm simulates a flood from selected seed points to delineate coherent plume structures. Parameters such as minimum distance between markers, thresholding for plume regions, and gradient-based seed selection are tuned to optimize segmentation across different sensors and models. The final output is a binary raster mask, with pixels labeled as 1 indicating methane-positive detections and zero representing methane-negative pixels. We experimented with doing the segmentation on marginal and conditional retrievals as well, but found that the binary probability map produced the best plume delineation. The implementation of the masking can be found in the methane-cv repo here: https://github.com/Orbio-Earth/Eucalyptus-code-archive/blob/main/methane-cv/sbr_2025/utils/prediction.py#L225 How do you estimate the emissions rate for a plume? Plume quantification is performed using the Integrated Mass Enhancement (IME) method, which is the standard approach for estimating methane emission rates from satellite observations, see Varon et al. (2018). IME represents the total methane enhancement within a segmented plume and is combined with wind data to estimate the source emission rate ( Q ). The core relationship is defined as: Q \\= IME / \u03c4 where \u03c4 is the effective residence time of methane within the detectable plume. This residence time is operationally defined as the plume length divided by wind speed, or: \u03c4 \\= L / U_eff Here, L is the effective plume length, calculated as the longest side of the bounding rectangle that encloses the segmented plume area, and U_eff is the effective wind speed, obtained from ERA5 or GEOSS meteorological reanalysis data at the time and location of the plume. The final formula used to estimate methane emissions, expressed in kilograms per hour, is: Q \\= (U_eff / L) \u00d7 IME \u00d7 57.75286 This equation incorporates the molecular weight of methane (16.04246 g/mol) and converts the emission rate from moles per second to kilograms per hour using the constant 57.75286. The IME itself is calculated by summing the methane enhancements (in mol/m\u00b2) across all pixels identified as plume-positive in the binary mask and multiplying by the pixel area (derived from the spatial resolution of the raster). Only pixels with enhancement values above zero are included. The raster used for this summation is the rescaled retrieval , which contains column methane values in physical units (the rescaled retrieval is explained in the section below). The quantification module in the methane-cv repo is located here: https://github.com/Orbio-Earth/Eucalyptus-code-archive/blob/main/methane-cv/src/inference Why do you use the rescaled retrieval for quantification? Classical methods for methane detection and emissions quantification directly produce a \"retrieval\" raster of methane concentration point estimates for each pixel in a scene. This same retrieval is used for masking and then quantification of the masked retrieval. With the computer vision approach, the output is probabilistic, with interpretation further discussed in the Data section. The prediction is in two parts: the binary probability and the conditional frac, which is converted into a conditional retrieval by inverting the radiative transfer laws. When it comes to masking, we have already discussed that we perform the masking on the binary probability channel, rather than the retrieval. But what is the analog of the masked retrieval used for quantification? There are two obvious options, but they both have significant downsides. Masking the conditional retrieval. Once a plume has been accepted, via a QA or automated process, the conditional retrieval answers the question \"if there is methane in this pixel, then how much?\" so it therefore seems natural to mask the conditional retrieval for quantification. But experiments with controlled releases showed that resulting quantifications significantly overestimate the ground truth emission rate, with a mean absolute percentage error measured above +50% in 8 controlled releases. This is because the tail of the plume, where the presence of methane above the threshold has low probability, gets treated as if it definitely contained methane. Visually, these masked plumes with boosted tails also have cliff edges at their perimeter, which is suggestive of the problem. Masking the marginal retrieval. The marginal retrieval is the conditional retrieval weighted by the binary probability, and is an analog to the output of the classical retrieval algorithms. By definition, it is lower than the conditional retrieval, and so results in lower quantifications. Visually, it also has smoother tails, without the cliff edge effect observed in the masked conditional retrieval. However, for a plume with low likelihood, the quantification is in effect weighted down by the model's probability of the plume not existing at all. One can think of this as a form of regularization. If such a plume is nevertheless selected in QA, this downweighting is inappropriate, and leads to underestimating the emission rate if the plume is real. It is also troubling that the binary probability is internally calibrated to the computer vision model's training data (more frequent plumes in training data \\= generally higher predicted probabilities), with no direct real-world interpretation, and this bias will translate directly to the quantification. The downsides of the conditional and marginal retrievals for quantification motivated the development of the rescaled retrieval , which is obtained after masking , by dividing the marginal retrieval by the maximum binary probability within each plume . This has several advantages: the tails of the plumes are not boosted inappropriately; there is no cliff edge at the perimeter of the plume; and the quantification is not highly sensitive to the frequency of plumes in the training data. There is an (admittedly fuzzy) probabilistic interpretation of the rescaled retrieval. The binary prediction for each pixel can be factored into two components: (1) the probability of the methane plume that this pixel belongs to being real, and (2) the conditional probability of this particular pixel exceeding the threshold given that the plume is real. By dividing by the maximum pixel probability within the plume, we are eliminating the first factor, because we are quantifying a plume assuming it is real. But the second factor remains, and tempers down the estimated concentrations in the more uncertain tails of the plume. What sources of wind data can be used? Any wind data source can be used, as long as it provides reliable wind speed and direction estimates near the surface, ideally with spatial resolution down to at least around 25km and temporal resolution of 1\u20133 hours. The better the resolution, the more accurate the emission quantification. We typically use GEOS-FP (NASA GMAO) and cross-check against ERA5 (ECMWF Reanalysis) . Both are freely available, provide global coverage, and offer spatial resolution of around 25km. GEOS-FP has a 3-hour temporal resolution, while ERA5 provides data hourly. While ERA5 is generally considered more accurate, in practice we\u2019ve found both datasets yield similar results for methane detection. The bigger opportunity for improvement would come from using higher-resolution wind data to better capture local wind behavior. That said, in a real-time production setting, ERA5 is often less practical due to slower availability and longer download times.","title":"Postprocessing"},{"location":"Postprocessing.html#postprocessing","text":"","title":"Postprocessing"},{"location":"Postprocessing.html#how-are-individual-plumes-masked-segmented-out-of-the-inference-output","text":"The masking stage in the methane detection pipeline is designed to convert sensor outputs into binary masks that isolate methane plume signals. Our masking is performed on a binary probability map that represents the likelihood of methane presence at each pixel. We use a watershed segmentation algorithm, a classical image processing technique that treats the probability map as a topographic surface. High-probability pixels are treated as peaks, and the algorithm simulates a flood from selected seed points to delineate coherent plume structures. Parameters such as minimum distance between markers, thresholding for plume regions, and gradient-based seed selection are tuned to optimize segmentation across different sensors and models. The final output is a binary raster mask, with pixels labeled as 1 indicating methane-positive detections and zero representing methane-negative pixels. We experimented with doing the segmentation on marginal and conditional retrievals as well, but found that the binary probability map produced the best plume delineation. The implementation of the masking can be found in the methane-cv repo here: https://github.com/Orbio-Earth/Eucalyptus-code-archive/blob/main/methane-cv/sbr_2025/utils/prediction.py#L225","title":"How are individual plumes masked (segmented) out of the inference output?"},{"location":"Postprocessing.html#how-do-you-estimate-the-emissions-rate-for-a-plume","text":"Plume quantification is performed using the Integrated Mass Enhancement (IME) method, which is the standard approach for estimating methane emission rates from satellite observations, see Varon et al. (2018). IME represents the total methane enhancement within a segmented plume and is combined with wind data to estimate the source emission rate ( Q ). The core relationship is defined as: Q \\= IME / \u03c4 where \u03c4 is the effective residence time of methane within the detectable plume. This residence time is operationally defined as the plume length divided by wind speed, or: \u03c4 \\= L / U_eff Here, L is the effective plume length, calculated as the longest side of the bounding rectangle that encloses the segmented plume area, and U_eff is the effective wind speed, obtained from ERA5 or GEOSS meteorological reanalysis data at the time and location of the plume. The final formula used to estimate methane emissions, expressed in kilograms per hour, is: Q \\= (U_eff / L) \u00d7 IME \u00d7 57.75286 This equation incorporates the molecular weight of methane (16.04246 g/mol) and converts the emission rate from moles per second to kilograms per hour using the constant 57.75286. The IME itself is calculated by summing the methane enhancements (in mol/m\u00b2) across all pixels identified as plume-positive in the binary mask and multiplying by the pixel area (derived from the spatial resolution of the raster). Only pixels with enhancement values above zero are included. The raster used for this summation is the rescaled retrieval , which contains column methane values in physical units (the rescaled retrieval is explained in the section below). The quantification module in the methane-cv repo is located here: https://github.com/Orbio-Earth/Eucalyptus-code-archive/blob/main/methane-cv/src/inference","title":"How do you estimate the emissions rate for a plume?"},{"location":"Postprocessing.html#why-do-you-use-the-rescaled-retrieval-for-quantification","text":"Classical methods for methane detection and emissions quantification directly produce a \"retrieval\" raster of methane concentration point estimates for each pixel in a scene. This same retrieval is used for masking and then quantification of the masked retrieval. With the computer vision approach, the output is probabilistic, with interpretation further discussed in the Data section. The prediction is in two parts: the binary probability and the conditional frac, which is converted into a conditional retrieval by inverting the radiative transfer laws. When it comes to masking, we have already discussed that we perform the masking on the binary probability channel, rather than the retrieval. But what is the analog of the masked retrieval used for quantification? There are two obvious options, but they both have significant downsides. Masking the conditional retrieval. Once a plume has been accepted, via a QA or automated process, the conditional retrieval answers the question \"if there is methane in this pixel, then how much?\" so it therefore seems natural to mask the conditional retrieval for quantification. But experiments with controlled releases showed that resulting quantifications significantly overestimate the ground truth emission rate, with a mean absolute percentage error measured above +50% in 8 controlled releases. This is because the tail of the plume, where the presence of methane above the threshold has low probability, gets treated as if it definitely contained methane. Visually, these masked plumes with boosted tails also have cliff edges at their perimeter, which is suggestive of the problem. Masking the marginal retrieval. The marginal retrieval is the conditional retrieval weighted by the binary probability, and is an analog to the output of the classical retrieval algorithms. By definition, it is lower than the conditional retrieval, and so results in lower quantifications. Visually, it also has smoother tails, without the cliff edge effect observed in the masked conditional retrieval. However, for a plume with low likelihood, the quantification is in effect weighted down by the model's probability of the plume not existing at all. One can think of this as a form of regularization. If such a plume is nevertheless selected in QA, this downweighting is inappropriate, and leads to underestimating the emission rate if the plume is real. It is also troubling that the binary probability is internally calibrated to the computer vision model's training data (more frequent plumes in training data \\= generally higher predicted probabilities), with no direct real-world interpretation, and this bias will translate directly to the quantification. The downsides of the conditional and marginal retrievals for quantification motivated the development of the rescaled retrieval , which is obtained after masking , by dividing the marginal retrieval by the maximum binary probability within each plume . This has several advantages: the tails of the plumes are not boosted inappropriately; there is no cliff edge at the perimeter of the plume; and the quantification is not highly sensitive to the frequency of plumes in the training data. There is an (admittedly fuzzy) probabilistic interpretation of the rescaled retrieval. The binary prediction for each pixel can be factored into two components: (1) the probability of the methane plume that this pixel belongs to being real, and (2) the conditional probability of this particular pixel exceeding the threshold given that the plume is real. By dividing by the maximum pixel probability within the plume, we are eliminating the first factor, because we are quantifying a plume assuming it is real. But the second factor remains, and tempers down the estimated concentrations in the more uncertain tails of the plume.","title":"Why do you use the rescaled retrieval for quantification?"},{"location":"Postprocessing.html#what-sources-of-wind-data-can-be-used","text":"Any wind data source can be used, as long as it provides reliable wind speed and direction estimates near the surface, ideally with spatial resolution down to at least around 25km and temporal resolution of 1\u20133 hours. The better the resolution, the more accurate the emission quantification. We typically use GEOS-FP (NASA GMAO) and cross-check against ERA5 (ECMWF Reanalysis) . Both are freely available, provide global coverage, and offer spatial resolution of around 25km. GEOS-FP has a 3-hour temporal resolution, while ERA5 provides data hourly. While ERA5 is generally considered more accurate, in practice we\u2019ve found both datasets yield similar results for methane detection. The bigger opportunity for improvement would come from using higher-resolution wind data to better capture local wind behavior. That said, in a real-time production setting, ERA5 is often less practical due to slower availability and longer download times.","title":"What sources of wind data can be used?"},{"location":"Postprocessing.html#_1","text":"","title":""},{"location":"Radiative_Transfer.html","text":"Radiative Transfer What is the model of radiative transfer that you use? Initially at Orbio we relied on a conventional multilayer radiative-transfer solver. Although such models are physically exact, running it for tens of millions of pixels needed in our synthetic-data pipeline became a hurdle. We therefore built a single-pass radiative transfer approximation that: (a) preserves the physics that matters for methane retrieval, (b) drops or simplifies the terms that are expensive to compute, but have negligible impact on retrieval accuracy. Due to the modular nature of the model, we can drop approximations one by one, trading off the speed for the accuracy of the model. How do we model the Top of Atmosphere (TOA) signal? In our model, similarly to other models we consider the TOA signal coming from the sunlight doubly passing through the atmosphere and reflecting off the surface of the earth. This can be modelled by considering the intensity recorded at the TOA, at the wavelength \\(\\lambda\\) \\[ I_\\lambda^{\\text{TOA}} = I_\\lambda^{\\text{sun}}\\, r_\\lambda\\, \\alpha_\\lambda\\, \\exp\\!\\Bigl[-\\!\\int_0^{\\infty} k_\\lambda(s')\\,ds'\\Bigr]. \\] , where - \\(I_\\lambda^{\\text{sun}}\\) - solar irradiance (Clough et al., 2005). - \\(r_\\lambda\\) \u2013 aerosol scattering factor (slowly varying with wavelength). - \\(\\alpha_\\lambda\\) \u2013 surface bidirectional reflectance (albedo). - \\(k_\\lambda\\) \u2013 aggregate volume-absorption coefficient of all gases. A multispectral detector does not see the monochromatic spectrum; each band \\(b\\) integrates the intensity through its spectral response function \\(f_\\lambda^{b}\\) : $$ I^{b}=\\int f_\\lambda^{b}\\,I_\\lambda^{\\text{TOA}}\\;d\\lambda . $$ It is worth noting here that predicting the exact TOA radiance is nearly an impossible task. Surface spectrum, aerosol load, and vertical gas profiles are poorly known on a per-pixel basis. Therefore we focus instead on a differential observable \\(nB\\) . Taking the ratio between the measured band radiance and the hypothetical methane-free radiance removes the first order dependence on the values of the parameters. \\[ nB = \\frac{I^{b}}{I^{b}_{0}} =\\int \\widehat n^{b}_{\\lambda}\\; \\exp\\!\\Bigl[-\\!\\int_0^{\\infty} k^{(\\Delta CH_4)}_{\\lambda}(s')\\,ds'\\Bigr] d\\lambda . \\] , where \\(k^{(\\Delta CH_4)}_{\\lambda}(s')\\) is the aggregate volume absorption coefficient in the hypothetical methane free scenario. \\(\\widehat n^{b}_{\\lambda}\\) defined as below $$ \\widehat n^{b} {\\lambda} =\\frac{f^{b} {\\lambda}\\,I_\\lambda^{\\text{sun}}\\,r_\\lambda\\alpha_\\lambda \\exp[-!\\int k^0_\\lambda ds']} {\\int f^{b} {\\lambda'}\\,I {\\lambda'}^{\\text{sun}}\\,r_{\\lambda'}\\alpha_{\\lambda'} \\exp[-!\\int k^0_{\\lambda'} ds']\\,d\\lambda'} . $$ Because the same unknowns appear in numerator and denominator, the ratio \\(nB\\) is first-order insensitive to errors in surface albedo, aerosol optical depth, and background gas profiles. \\(\\widehat n^{b}_{\\lambda}\\) functions as a wavelength-dependent weighting mask that tells us which parts of the methane absorption spectrum dominate the band signal. So what is lightweight about your calculation? A formal error budget showed that one of the dominant uncertainties (20%) in full multilayer radiative transfer model of the differential signal comes from the ignorance of surface reflectance spectrum, not from the fine details of atmospheric propagation. This is fundamentally very difficult to know a priori. By accepting that there exist an error floor and simplifying everything that contributes less than it, we cut runtime by two orders of magnitude while keeping total retrieval error within the same 20% envelope. Making these simplifications, we treat the entire atmosphere as a single slab at 300 K and 1 bar. Absorptivities for CH\u2084, CO\u2082, H\u2082O, and the other relevant gases are taken from the HITRAN database at those conditions, while the background column densities follow the U.S. Standard Atmosphere (1976) scaled to present-day values (about 421 ppm CO\u2082 and 1900 ppb CH\u2084). Because the model uses a single slab, the altitude dependence of the absorption coefficients is ignored; the same coefficients act uniformly from the surface to the top of the atmosphere, so no layer-by-layer integration is required. We also omit explicit treatment of surface albedo and the slow aerosol spectral slope. Both vary only gently across the SWIR methane windows and together dominate the irreducible 20% uncertainty that already limits the retrieval. Their combined effect is carried as a fixed term in the error budget. Finally, the global filter functions \\(\\\\widehat n^{b}\\_{\\\\lambda}\\) are pre-computed offline and stored with the code, so a run-time evaluation reduces to a quick dot product plus a lookup in the methane cross-section table. If future work reduces the albedo uncertainty, the framework lets you dial the radiative transfer fidelity back up and reclaim those accuracy gains How do we impute the data into the synthetic data? To inject synthetic methane plumes into a scene we proceed as follows. First, we generate a lookup table that gives the global-filter factor \\(nB\\) for each band as a function of the excess methane column \\(\\\\Delta X\\_{CH\\_4}\\) value to build an n-dimensional array of attenuation factors - our \u201cplume-normalised brightness\u201d data cube. Finally, we obtain the synthetic-plume radiance cube simply by multiplying this attenuation cube element-wise with the original radiance cube of the methane-free scene. This is visualised in the diagram below.","title":"Radiative transfer"},{"location":"Radiative_Transfer.html#radiative-transfer","text":"","title":"Radiative Transfer"},{"location":"Radiative_Transfer.html#what-is-the-model-of-radiative-transfer-that-you-use","text":"Initially at Orbio we relied on a conventional multilayer radiative-transfer solver. Although such models are physically exact, running it for tens of millions of pixels needed in our synthetic-data pipeline became a hurdle. We therefore built a single-pass radiative transfer approximation that: (a) preserves the physics that matters for methane retrieval, (b) drops or simplifies the terms that are expensive to compute, but have negligible impact on retrieval accuracy. Due to the modular nature of the model, we can drop approximations one by one, trading off the speed for the accuracy of the model.","title":"What is the model of radiative transfer that you use?"},{"location":"Radiative_Transfer.html#how-do-we-model-the-top-of-atmosphere-toa-signal","text":"In our model, similarly to other models we consider the TOA signal coming from the sunlight doubly passing through the atmosphere and reflecting off the surface of the earth. This can be modelled by considering the intensity recorded at the TOA, at the wavelength \\(\\lambda\\) \\[ I_\\lambda^{\\text{TOA}} = I_\\lambda^{\\text{sun}}\\, r_\\lambda\\, \\alpha_\\lambda\\, \\exp\\!\\Bigl[-\\!\\int_0^{\\infty} k_\\lambda(s')\\,ds'\\Bigr]. \\] , where - \\(I_\\lambda^{\\text{sun}}\\) - solar irradiance (Clough et al., 2005). - \\(r_\\lambda\\) \u2013 aerosol scattering factor (slowly varying with wavelength). - \\(\\alpha_\\lambda\\) \u2013 surface bidirectional reflectance (albedo). - \\(k_\\lambda\\) \u2013 aggregate volume-absorption coefficient of all gases. A multispectral detector does not see the monochromatic spectrum; each band \\(b\\) integrates the intensity through its spectral response function \\(f_\\lambda^{b}\\) : $$ I^{b}=\\int f_\\lambda^{b}\\,I_\\lambda^{\\text{TOA}}\\;d\\lambda . $$ It is worth noting here that predicting the exact TOA radiance is nearly an impossible task. Surface spectrum, aerosol load, and vertical gas profiles are poorly known on a per-pixel basis. Therefore we focus instead on a differential observable \\(nB\\) . Taking the ratio between the measured band radiance and the hypothetical methane-free radiance removes the first order dependence on the values of the parameters. \\[ nB = \\frac{I^{b}}{I^{b}_{0}} =\\int \\widehat n^{b}_{\\lambda}\\; \\exp\\!\\Bigl[-\\!\\int_0^{\\infty} k^{(\\Delta CH_4)}_{\\lambda}(s')\\,ds'\\Bigr] d\\lambda . \\] , where \\(k^{(\\Delta CH_4)}_{\\lambda}(s')\\) is the aggregate volume absorption coefficient in the hypothetical methane free scenario. \\(\\widehat n^{b}_{\\lambda}\\) defined as below $$ \\widehat n^{b} {\\lambda} =\\frac{f^{b} {\\lambda}\\,I_\\lambda^{\\text{sun}}\\,r_\\lambda\\alpha_\\lambda \\exp[-!\\int k^0_\\lambda ds']} {\\int f^{b} {\\lambda'}\\,I {\\lambda'}^{\\text{sun}}\\,r_{\\lambda'}\\alpha_{\\lambda'} \\exp[-!\\int k^0_{\\lambda'} ds']\\,d\\lambda'} . $$ Because the same unknowns appear in numerator and denominator, the ratio \\(nB\\) is first-order insensitive to errors in surface albedo, aerosol optical depth, and background gas profiles. \\(\\widehat n^{b}_{\\lambda}\\) functions as a wavelength-dependent weighting mask that tells us which parts of the methane absorption spectrum dominate the band signal.","title":"How do we model the Top of Atmosphere (TOA) signal?"},{"location":"Radiative_Transfer.html#so-what-is-lightweight-about-your-calculation","text":"A formal error budget showed that one of the dominant uncertainties (20%) in full multilayer radiative transfer model of the differential signal comes from the ignorance of surface reflectance spectrum, not from the fine details of atmospheric propagation. This is fundamentally very difficult to know a priori. By accepting that there exist an error floor and simplifying everything that contributes less than it, we cut runtime by two orders of magnitude while keeping total retrieval error within the same 20% envelope. Making these simplifications, we treat the entire atmosphere as a single slab at 300 K and 1 bar. Absorptivities for CH\u2084, CO\u2082, H\u2082O, and the other relevant gases are taken from the HITRAN database at those conditions, while the background column densities follow the U.S. Standard Atmosphere (1976) scaled to present-day values (about 421 ppm CO\u2082 and 1900 ppb CH\u2084). Because the model uses a single slab, the altitude dependence of the absorption coefficients is ignored; the same coefficients act uniformly from the surface to the top of the atmosphere, so no layer-by-layer integration is required. We also omit explicit treatment of surface albedo and the slow aerosol spectral slope. Both vary only gently across the SWIR methane windows and together dominate the irreducible 20% uncertainty that already limits the retrieval. Their combined effect is carried as a fixed term in the error budget. Finally, the global filter functions \\(\\\\widehat n^{b}\\_{\\\\lambda}\\) are pre-computed offline and stored with the code, so a run-time evaluation reduces to a quick dot product plus a lookup in the methane cross-section table. If future work reduces the albedo uncertainty, the framework lets you dial the radiative transfer fidelity back up and reclaim those accuracy gains","title":"So what is lightweight about your calculation?"},{"location":"Radiative_Transfer.html#how-do-we-impute-the-data-into-the-synthetic-data","text":"To inject synthetic methane plumes into a scene we proceed as follows. First, we generate a lookup table that gives the global-filter factor \\(nB\\) for each band as a function of the excess methane column \\(\\\\Delta X\\_{CH\\_4}\\) value to build an n-dimensional array of attenuation factors - our \u201cplume-normalised brightness\u201d data cube. Finally, we obtain the synthetic-plume radiance cube simply by multiplying this attenuation cube element-wise with the original radiance cube of the methane-free scene. This is visualised in the diagram below.","title":"How do we impute the data into the synthetic data?"},{"location":"Radiative_Transfer.html#_1","text":"","title":""},{"location":"Radiative_Transfer.html#_2","text":"","title":""},{"location":"Radiative_Transfer.html#_3","text":"","title":""},{"location":"Validation.html","text":"Validation How do you assess model performance? We assess model performance through a combination of synthetic plumes, ground truth emissions, and metrics tailored to production constraints, such as the detection threshold . These constraints reflect the trade-off we face in production between high detection sensitivity and keeping false positives low enough to avoid overwhelming downstream QA and users. During training, we minimize a custom loss function that balances methane detection (via binary classification) and quantification (via conditional regression). See here for more details on the loss function. The validation dataset consists of synthetic plumes injected into real satellite scenes across three representative regions: Hassi/Permian/Marcellus . At regular intervals during training, we evaluate model performance on this synthetic validation dataset. We select the best checkpoint based on recall, computed at a detection threshold that is tuned to meet a fixed false positive constraint. See here for more details. In addition to synthetic evaluation, we test model performance against a curated list of real world ground truth emission events compiled from various studies, including Varon et al. (2021), IMEO Notified Plumes (reported high-emission events from the UN\u2019s International Methane Emissions Observatory), and SBR field campaigns (Single-Blind Release studies conducted by Stanford that provide known, independently quantified plumes under controlled conditions). These include known plumes with verified emission rates across sites in Algeria, Turkmenistan, the US, and more. Ground truth checks help confirm the model can generalize to real data and not just synthetic proxies. What are the results of your best model? Our best model achieves high sensitivity to methane plumes while maintaining strict false positive controls. Here we summarize the detection threshold (DT) results from synthetic plumes and performance on known true positive cases (ground truth). Detection Threshold Results The model\u2019s sensitivity varies by geography, reflecting differences in scene noise, terrain, and atmospheric conditions. For each region, we report the emission rate that is detected in 10%, 50%, and 90% of cases (=probability of detection (POD) levels). Hassi (Algeria) We detect emissions of 260 kg/hr in 10% of cases 50% DT: 620 kg/hr 90% DT: 1593 kg/hr Permian (USA) 10% DT: 666 kg/hr 50% DT: 1776 kg/hr 90% DT: 5753 kg/hr Marcellus (USA) 10% DT: 2344 kg/hr 50% DT: 8535 kg/hr 90% DT: >25000 kg/hr These thresholds reflect the emission sizes the model can detect at each confidence level, under a controlled false positive rate of 2 FP plumes per 500x500 px area. These results directly inform the model\u2019s utility in operational settings, by helping users understand the minimum emission rates they can expect the model to reliably capture. In practice, this means that in regions like the Permian, the model is likely to detect most large emitter events but may miss smaller emissions below around 600\u202fkg/h. While in Hassi-like regions, the model is more sensitive and can pick up quite low emissions. Ground Truth Dataset (Single Blind Release) Performance The model\u2019s performance on ground truth data was also promising. Here we showcase its performance on the Single Blind Release (SBR) site at Casa Grande. From the 2022 and 2024 campaigns, there were 35 controlled events: 13 methane releases and 22 no-release scenes. The model achieved: 10 true positives 22 true negatives 0 false positives 3 false negatives This performance indicates strong real-world detection capability at the Casa Grande site, with high precision and only a few missed detections. The complete absence of false positives is especially promising, suggesting good robustness to background noise in clean scenes. However, the three missed plumes highlight that lower-emission or visually subtle releases can still be challenging under certain conditions. Below, we show three representative examples to illustrate these outcomes: This is an example of a high confidence true positive , where the model cleanly detects the release. The expected emission rate was 1358 kg/hr and we quantified 1509 kg/hr. This is an example of another true positive but with nearby false positives . The actual emission was 628 kg/hr, we detected 900 kg/hr. Lastly, this is an example of a missed detection . Although this was a medium sized emission event (763 kg/hr), our model was not able to pick it up. This could be explained by the fact there is a slight haze over the emission site, which hinders the model\u2019s ability to detect. Why have you selected these three specific regions to validate in? These regions allow us to evaluate our model\u2019s generalizability and false positive rate (FPR) performance under varying environmental conditions, specifically in areas we actively run our models (major oil and gas producing areas). Each region maps to a distinct Signal-to-Noise Ratio (SNR) tier, allowing us to benchmark model behavior across easy, medium, and hard detection settings. Tier 1: Easy \u2014 Hassi Messaoud (Algeria) Environment: Desert-like, minimal vegetation Rationale: This region offers high SNR conditions that are ideal for detecting methane. It serves as a \u201cbest-case\u201d environment, allowing us to establish a baseline FPR under clean, low-noise conditions Tier 2: Medium \u2014 Permian Basin (USA) Environment: Mixed vegetation, brush, sparse vegetation Rationale: The Permian is a high-emission oil and gas region with moderate background complexity and reflectance It represents a moderate SNR \u2014 realistic for detection but still prone to moderate false positives Tier 3: Hard \u2014 Marcellus Shale (USA) Environment: Forested, agricultural, hilly terrain Rationale: Marcellus provides the most challenging conditions for methane detection due to its low SNR and complex backgrounds. It defines the model\u2019s worst-case false positive behavior and sets realistic expectations for performance in difficult scenes. Validating in this region stress-tests our model\u2019s robustness What metrics do you use during model training? As the main validation metric, we selected average Recall over multiple emission buckets and averaged over the three regions Hassi/Permian/Marcellus . Recall is calculated using a classification threshold that allows for 10 false positive pixels on average over all 128x128 validation crops. Balancing False Positives and Recall allows us to select the model that offers the highest sensitivity (i.e. best detection accuracy) without exceeding our target noise tolerance. We have found that performance of the regression part of the model correlates highly with the performance of the binary part of the model. See here for the implementation of validation. We also output metric visualizations after each validation epoch. The False Positive Px Rate Curves show the Likelihood threshold (which is the binary threshold * 10) that was selected to give 10px on average over all validation chips and all three regions. Given this threshold, we calculate the Recall (Detection Probability) for different emission rate buckets for the three regions. Averaging over emission rates and regions gives us the final average Recall value we track for overall performance. For the below example we get: mAvgRecall: 42.7% (Hassi: 72.9%, Marcellus: 10.8%, Permian: 44.4%) How do you estimate the detection threshold of your model? To get to an answer to the question \u201cwe can detect plumes of X kg/hr in Y% of cases\u201d we developed the following detection threshold method, which we run on models we want to release to production. This method is approximated during training as described here . We have a dataset of simulated methane plumes produced by Gorro\u00f1o et al. (2023) using large eddy simulations. For each plume, there is a raster of concentration, and a flow rate at the source. We can scale a plume up and down as the concentrations are proportional to the flow rate. Using this, taking a single plume in a single scene, we can insert a plume with a known emission rate into the target scene, and see if a model can detect it. The computer vision models return a likelihood score, so rather than a yes or no answer, we can look at the likelihood score (more precisely: the maximum of the likelihood score within the footprint of the plume) as the plume gets stronger. We can produce a graph that looks like this. Taking this one step further, still for a single tile or crop, we can use all of the simulations we have, and randomly move and rotate them around the scene, still with a range of intensities. Imagine getting hundreds of curves like the one above. We can then start making probabilistic statements about detections in this scene. If we choose a likelihood score threshold for detections (say, we accept all plumes above 4/10), then we can calculate the percentage of detections for every plume size. We can now make statements like \"a plume of size 1,500 kg/hr would have a 40% probability of being detected by our model near this asset on this date.\" Graphically, it would look something like: For any choice of detection threshold, we get a different curve. We can put all of them on the same graph. This is still for a single site on one date. The last step is to repeat this exercise for an entire area of interest (e.g. the Permian basin), select a Likelihood Score that yields an acceptable number of False Positives and to report the emission rates we can detect in 10%/50%/90% of cases. For finding the Likelihood Score with an acceptable number of False Positive plume detections, we can run the model over many chips without methane and plot the relationship between Threshold (= used interchangeably with Likelihood Score, LS is just scaled to lie between 0-10 whereas Threshold/Marker Threshold lie between 0-1) and average False Positive plumes (connected plume objects, not pixels). Below, this was normalized to produce an average of 2 False Positive plumes averaged over the three regions per 500x500 px tile, because we used 500x500 px tiles in production. The magenta line at FP=2 gives us a Likelihood Threshold of 3.044 which we can use to create our final mean Detection Curves. For Hassi, we detect emissions of 260 kg/h in 10% of cases, 620 kg/h in 50% and 1593 kg/h in 90%. For Permian, we detect emissions of 666 kg/h in 10% of cases, 1776 kg/h in 50% and 5753 kg/h in 90%. For Marcellus, we detect emissions of 2344 kg/h in 10% of cases, 8535 kg/h in 50% and >25000 kg/h in 90%. Does strong performance on real world test data (e.g. ground truth plumes) mean the model is ready for production? Not necessarily. Methane detection in satellite imagery is fundamentally difficult. Plumes can be faint, variable, and easily confounded by clouds, terrain, haze, and sensor noise. Even when the model performs well on real emission events, this doesn\u2019t guarantee readiness for automated, wide-scale deployment\u2014especially given how limited and noisy real ground truth data typically is. Single Blind Release (SBR) studies offer a rigorous benchmark: methane is released at known times and rates while modelers remain blinded, allowing for objective validation against high confidence ground truth data. These studies typically occur in clean desert environments, which reduces ambiguity from vegetation. But even under these favourable conditions, models can fail to detect low rate emissions or struggle with false positives. This highlights just how sensitive and error-prone the detection task can be. Moreover, production use requires much more than success on curated cases. The model must: Operate across millions of diverse, uncurated scenes with no prior hints of where emissions may occur Generalize across regions, seasons, and image quality Maintain strict false positive controls to avoid overwhelming QA That\u2019s why strong performance on real-world test cases like SBRs is necessary\u2014but not sufficient. We supplement them with synthetic plumes, regional validation datasets, and controlled FPR benchmarking to assess model reliability under realistic, scalable operational conditions. References Gorro\u00f1o, Javier, Daniel J. Varon, Itziar Irakulis-Loitxate, and Luis Guanter. \"Understanding the potential of Sentinel-2 for monitoring methane point emissions.\" Atmospheric Measurement Techniques 16, no. 1 (2023): 89-107. Varon, Daniel J., Daniel J. Jacob, Jason McKeever, Dylan Jervis, Berke OA Durak, Yan Xia, and Yi Huang. \"Quantifying methane point sources from fine-scale satellite observations of atmospheric methane plumes.\" Atmospheric Measurement Techniques 11, no. 10 (2018): 5673-5686. Varon, Daniel J., Dylan Jervis, Jason McKeever, Ian Spence, David Gains, and Daniel J. Jacob. \"High-frequency monitoring of anomalous methane point sources with multispectral Sentinel-2 satellite observations.\" Atmospheric Measurement Techniques 14, no. 4 (2021).","title":"Validation"},{"location":"Validation.html#validation","text":"","title":"Validation"},{"location":"Validation.html#how-do-you-assess-model-performance","text":"We assess model performance through a combination of synthetic plumes, ground truth emissions, and metrics tailored to production constraints, such as the detection threshold . These constraints reflect the trade-off we face in production between high detection sensitivity and keeping false positives low enough to avoid overwhelming downstream QA and users. During training, we minimize a custom loss function that balances methane detection (via binary classification) and quantification (via conditional regression). See here for more details on the loss function. The validation dataset consists of synthetic plumes injected into real satellite scenes across three representative regions: Hassi/Permian/Marcellus . At regular intervals during training, we evaluate model performance on this synthetic validation dataset. We select the best checkpoint based on recall, computed at a detection threshold that is tuned to meet a fixed false positive constraint. See here for more details. In addition to synthetic evaluation, we test model performance against a curated list of real world ground truth emission events compiled from various studies, including Varon et al. (2021), IMEO Notified Plumes (reported high-emission events from the UN\u2019s International Methane Emissions Observatory), and SBR field campaigns (Single-Blind Release studies conducted by Stanford that provide known, independently quantified plumes under controlled conditions). These include known plumes with verified emission rates across sites in Algeria, Turkmenistan, the US, and more. Ground truth checks help confirm the model can generalize to real data and not just synthetic proxies.","title":"How do you assess model performance?"},{"location":"Validation.html#what-are-the-results-of-your-best-model","text":"Our best model achieves high sensitivity to methane plumes while maintaining strict false positive controls. Here we summarize the detection threshold (DT) results from synthetic plumes and performance on known true positive cases (ground truth).","title":"What are the results of your best model?"},{"location":"Validation.html#detection-threshold-results","text":"The model\u2019s sensitivity varies by geography, reflecting differences in scene noise, terrain, and atmospheric conditions. For each region, we report the emission rate that is detected in 10%, 50%, and 90% of cases (=probability of detection (POD) levels). Hassi (Algeria) We detect emissions of 260 kg/hr in 10% of cases 50% DT: 620 kg/hr 90% DT: 1593 kg/hr Permian (USA) 10% DT: 666 kg/hr 50% DT: 1776 kg/hr 90% DT: 5753 kg/hr Marcellus (USA) 10% DT: 2344 kg/hr 50% DT: 8535 kg/hr 90% DT: >25000 kg/hr These thresholds reflect the emission sizes the model can detect at each confidence level, under a controlled false positive rate of 2 FP plumes per 500x500 px area. These results directly inform the model\u2019s utility in operational settings, by helping users understand the minimum emission rates they can expect the model to reliably capture. In practice, this means that in regions like the Permian, the model is likely to detect most large emitter events but may miss smaller emissions below around 600\u202fkg/h. While in Hassi-like regions, the model is more sensitive and can pick up quite low emissions.","title":"Detection Threshold Results"},{"location":"Validation.html#ground-truth-dataset-single-blind-release-performance","text":"The model\u2019s performance on ground truth data was also promising. Here we showcase its performance on the Single Blind Release (SBR) site at Casa Grande. From the 2022 and 2024 campaigns, there were 35 controlled events: 13 methane releases and 22 no-release scenes. The model achieved: 10 true positives 22 true negatives 0 false positives 3 false negatives This performance indicates strong real-world detection capability at the Casa Grande site, with high precision and only a few missed detections. The complete absence of false positives is especially promising, suggesting good robustness to background noise in clean scenes. However, the three missed plumes highlight that lower-emission or visually subtle releases can still be challenging under certain conditions. Below, we show three representative examples to illustrate these outcomes: This is an example of a high confidence true positive , where the model cleanly detects the release. The expected emission rate was 1358 kg/hr and we quantified 1509 kg/hr. This is an example of another true positive but with nearby false positives . The actual emission was 628 kg/hr, we detected 900 kg/hr. Lastly, this is an example of a missed detection . Although this was a medium sized emission event (763 kg/hr), our model was not able to pick it up. This could be explained by the fact there is a slight haze over the emission site, which hinders the model\u2019s ability to detect.","title":"Ground Truth Dataset (Single Blind Release) Performance"},{"location":"Validation.html#why-have-you-selected-these-three-specific-regions-to-validate-in","text":"These regions allow us to evaluate our model\u2019s generalizability and false positive rate (FPR) performance under varying environmental conditions, specifically in areas we actively run our models (major oil and gas producing areas). Each region maps to a distinct Signal-to-Noise Ratio (SNR) tier, allowing us to benchmark model behavior across easy, medium, and hard detection settings. Tier 1: Easy \u2014 Hassi Messaoud (Algeria) Environment: Desert-like, minimal vegetation Rationale: This region offers high SNR conditions that are ideal for detecting methane. It serves as a \u201cbest-case\u201d environment, allowing us to establish a baseline FPR under clean, low-noise conditions Tier 2: Medium \u2014 Permian Basin (USA) Environment: Mixed vegetation, brush, sparse vegetation Rationale: The Permian is a high-emission oil and gas region with moderate background complexity and reflectance It represents a moderate SNR \u2014 realistic for detection but still prone to moderate false positives Tier 3: Hard \u2014 Marcellus Shale (USA) Environment: Forested, agricultural, hilly terrain Rationale: Marcellus provides the most challenging conditions for methane detection due to its low SNR and complex backgrounds. It defines the model\u2019s worst-case false positive behavior and sets realistic expectations for performance in difficult scenes. Validating in this region stress-tests our model\u2019s robustness","title":"Why have you selected these three specific regions to validate in?"},{"location":"Validation.html#what-metrics-do-you-use-during-model-training","text":"As the main validation metric, we selected average Recall over multiple emission buckets and averaged over the three regions Hassi/Permian/Marcellus . Recall is calculated using a classification threshold that allows for 10 false positive pixels on average over all 128x128 validation crops. Balancing False Positives and Recall allows us to select the model that offers the highest sensitivity (i.e. best detection accuracy) without exceeding our target noise tolerance. We have found that performance of the regression part of the model correlates highly with the performance of the binary part of the model. See here for the implementation of validation. We also output metric visualizations after each validation epoch. The False Positive Px Rate Curves show the Likelihood threshold (which is the binary threshold * 10) that was selected to give 10px on average over all validation chips and all three regions. Given this threshold, we calculate the Recall (Detection Probability) for different emission rate buckets for the three regions. Averaging over emission rates and regions gives us the final average Recall value we track for overall performance. For the below example we get: mAvgRecall: 42.7% (Hassi: 72.9%, Marcellus: 10.8%, Permian: 44.4%)","title":"What metrics do you use during model training?"},{"location":"Validation.html#how-do-you-estimate-the-detection-threshold-of-your-model","text":"To get to an answer to the question \u201cwe can detect plumes of X kg/hr in Y% of cases\u201d we developed the following detection threshold method, which we run on models we want to release to production. This method is approximated during training as described here . We have a dataset of simulated methane plumes produced by Gorro\u00f1o et al. (2023) using large eddy simulations. For each plume, there is a raster of concentration, and a flow rate at the source. We can scale a plume up and down as the concentrations are proportional to the flow rate. Using this, taking a single plume in a single scene, we can insert a plume with a known emission rate into the target scene, and see if a model can detect it. The computer vision models return a likelihood score, so rather than a yes or no answer, we can look at the likelihood score (more precisely: the maximum of the likelihood score within the footprint of the plume) as the plume gets stronger. We can produce a graph that looks like this. Taking this one step further, still for a single tile or crop, we can use all of the simulations we have, and randomly move and rotate them around the scene, still with a range of intensities. Imagine getting hundreds of curves like the one above. We can then start making probabilistic statements about detections in this scene. If we choose a likelihood score threshold for detections (say, we accept all plumes above 4/10), then we can calculate the percentage of detections for every plume size. We can now make statements like \"a plume of size 1,500 kg/hr would have a 40% probability of being detected by our model near this asset on this date.\" Graphically, it would look something like: For any choice of detection threshold, we get a different curve. We can put all of them on the same graph. This is still for a single site on one date. The last step is to repeat this exercise for an entire area of interest (e.g. the Permian basin), select a Likelihood Score that yields an acceptable number of False Positives and to report the emission rates we can detect in 10%/50%/90% of cases. For finding the Likelihood Score with an acceptable number of False Positive plume detections, we can run the model over many chips without methane and plot the relationship between Threshold (= used interchangeably with Likelihood Score, LS is just scaled to lie between 0-10 whereas Threshold/Marker Threshold lie between 0-1) and average False Positive plumes (connected plume objects, not pixels). Below, this was normalized to produce an average of 2 False Positive plumes averaged over the three regions per 500x500 px tile, because we used 500x500 px tiles in production. The magenta line at FP=2 gives us a Likelihood Threshold of 3.044 which we can use to create our final mean Detection Curves. For Hassi, we detect emissions of 260 kg/h in 10% of cases, 620 kg/h in 50% and 1593 kg/h in 90%. For Permian, we detect emissions of 666 kg/h in 10% of cases, 1776 kg/h in 50% and 5753 kg/h in 90%. For Marcellus, we detect emissions of 2344 kg/h in 10% of cases, 8535 kg/h in 50% and >25000 kg/h in 90%.","title":"How do you estimate the detection threshold of your model?"},{"location":"Validation.html#does-strong-performance-on-real-world-test-data-eg-ground-truth-plumes-mean-the-model-is-ready-for-production","text":"Not necessarily. Methane detection in satellite imagery is fundamentally difficult. Plumes can be faint, variable, and easily confounded by clouds, terrain, haze, and sensor noise. Even when the model performs well on real emission events, this doesn\u2019t guarantee readiness for automated, wide-scale deployment\u2014especially given how limited and noisy real ground truth data typically is. Single Blind Release (SBR) studies offer a rigorous benchmark: methane is released at known times and rates while modelers remain blinded, allowing for objective validation against high confidence ground truth data. These studies typically occur in clean desert environments, which reduces ambiguity from vegetation. But even under these favourable conditions, models can fail to detect low rate emissions or struggle with false positives. This highlights just how sensitive and error-prone the detection task can be. Moreover, production use requires much more than success on curated cases. The model must: Operate across millions of diverse, uncurated scenes with no prior hints of where emissions may occur Generalize across regions, seasons, and image quality Maintain strict false positive controls to avoid overwhelming QA That\u2019s why strong performance on real-world test cases like SBRs is necessary\u2014but not sufficient. We supplement them with synthetic plumes, regional validation datasets, and controlled FPR benchmarking to assess model reliability under realistic, scalable operational conditions.","title":"Does strong performance on real world test data (e.g. ground truth plumes) mean the model is ready for production?"},{"location":"Validation.html#references","text":"Gorro\u00f1o, Javier, Daniel J. Varon, Itziar Irakulis-Loitxate, and Luis Guanter. \"Understanding the potential of Sentinel-2 for monitoring methane point emissions.\" Atmospheric Measurement Techniques 16, no. 1 (2023): 89-107. Varon, Daniel J., Daniel J. Jacob, Jason McKeever, Dylan Jervis, Berke OA Durak, Yan Xia, and Yi Huang. \"Quantifying methane point sources from fine-scale satellite observations of atmospheric methane plumes.\" Atmospheric Measurement Techniques 11, no. 10 (2018): 5673-5686. Varon, Daniel J., Dylan Jervis, Jason McKeever, Ian Spence, David Gains, and Daniel J. Jacob. \"High-frequency monitoring of anomalous methane point sources with multispectral Sentinel-2 satellite observations.\" Atmospheric Measurement Techniques 14, no. 4 (2021).","title":"References"}]}